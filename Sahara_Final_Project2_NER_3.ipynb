{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7wfLWyYkvDi"
      },
      "source": [
        "# BERT Fine-Tuning for named-entity recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXkJVGybEoYN"
      },
      "source": [
        "## Importing Python Libraries and preparing the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4_YJqjR_Gjw",
        "outputId": "f8dd68ab-5c05-49ff-d692-3d012ebfebc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: transformers in /lustre/fs1/home/ssheikholeslami/.local/lib/python3.8/site-packages (4.39.1)\n",
            "Requirement already satisfied: seqeval[gpu] in /lustre/fs1/home/ssheikholeslami/.local/lib/python3.8/site-packages (1.2.2)\n",
            "Requirement already satisfied: filelock in /apps/anaconda/anaconda3/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /lustre/fs1/home/ssheikholeslami/.local/lib/python3.8/site-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /lustre/fs1/home/ssheikholeslami/.local/lib/python3.8/site-packages (from transformers) (1.24.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /lustre/fs1/home/ssheikholeslami/.local/lib/python3.8/site-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /lustre/fs1/home/ssheikholeslami/.local/lib/python3.8/site-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /apps/anaconda/anaconda3/lib/python3.8/site-packages (from transformers) (4.50.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /apps/anaconda/anaconda3/lib/python3.8/site-packages (from transformers) (5.3.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /apps/anaconda/anaconda3/lib/python3.8/site-packages (from transformers) (2020.9.27)\n",
            "Requirement already satisfied: packaging>=20.0 in /lustre/fs1/home/ssheikholeslami/.local/lib/python3.8/site-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: requests in /apps/anaconda/anaconda3/lib/python3.8/site-packages (from transformers) (2.24.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /apps/anaconda/anaconda3/lib/python3.8/site-packages (from seqeval[gpu]) (0.23.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /apps/anaconda/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /lustre/fs1/home/ssheikholeslami/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /apps/anaconda/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /apps/anaconda/anaconda3/lib/python3.8/site-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /apps/anaconda/anaconda3/lib/python3.8/site-packages (from requests->transformers) (1.25.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /apps/anaconda/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /apps/anaconda/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (2.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /apps/anaconda/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (0.17.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /apps/anaconda/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.5.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers seqeval[gpu]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IEnlUbgm8z3B"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertConfig, BertForTokenClassification\n",
        "from seqeval.metrics import classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS7okZC2Ezux"
      },
      "source": [
        "### Insuring GPU Is enables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sm1krxJtKxpx",
        "outputId": "a082c3ab-e489-4781-9b4d-4533ca1ff9f7",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5J2Lc9Z8FBBT"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9lqPMeSFFNT"
      },
      "source": [
        "### Loading NER dataset from [Kaggle](https://www.kaggle.com/datasets/namanj27/ner-dataset?resource=download)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "h03OT7z5FAc5"
      },
      "outputs": [],
      "source": [
        "# Commented out as database will be utilize locally - Uncomment to load from Sahara's Drive\n",
        "# #mounting google drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aLgbzN4lFLZZ"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.chdir('/content/drive/MyDrive/NLP')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "deLB9HVX5I6F"
      },
      "outputs": [],
      "source": [
        "# load data into pd from this file\n",
        "# data = pd.read_csv(\"ner_datasetreference.csv\", encoding='unicode_escape')\n",
        "\n",
        "# Use file locally\n",
        "data = pd.read_csv(\"ner_datasetreference.csv\", encoding='unicode_escape')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4nSfJlnFnQv"
      },
      "source": [
        "### **Exploratory Data Analysis (EDA)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol0zaTdZF7Mp"
      },
      "source": [
        "### **1. Data Description:**\n",
        "\n",
        "# The provided dataset contains information about named entities in text data. Each row represents a word, with the following columns:\n",
        "\n",
        "### - **Sentence #:** Sentence identifier.\n",
        "### - **Word:** The actual word.\n",
        "### - **POS:** Part-of-speech tag.\n",
        "### - **Tag:** Named entity tag (e.g., PERSON, ORGANIZATION, LOCATION)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "yXOD9nsNFw87",
        "outputId": "264a1add-cf97-4439-bf00-a28605afe037"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence #</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>of</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>have</td>\n",
              "      <td>VBP</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>marched</td>\n",
              "      <td>VBN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Sentence #           Word  POS Tag\n",
              "0  Sentence: 1      Thousands  NNS   O\n",
              "1          NaN             of   IN   O\n",
              "2          NaN  demonstrators  NNS   O\n",
              "3          NaN           have  VBP   O\n",
              "4          NaN        marched  VBN   O"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "gGW2QXUQFbWF",
        "outputId": "09b16e2b-f14e-44d5-b47b-53ac810a3e47"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence #</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1048570</th>\n",
              "      <td>NaN</td>\n",
              "      <td>they</td>\n",
              "      <td>PRP</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048571</th>\n",
              "      <td>NaN</td>\n",
              "      <td>responded</td>\n",
              "      <td>VBD</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048572</th>\n",
              "      <td>NaN</td>\n",
              "      <td>to</td>\n",
              "      <td>TO</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048573</th>\n",
              "      <td>NaN</td>\n",
              "      <td>the</td>\n",
              "      <td>DT</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048574</th>\n",
              "      <td>NaN</td>\n",
              "      <td>attack</td>\n",
              "      <td>NN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Sentence #       Word  POS Tag\n",
              "1048570        NaN       they  PRP   O\n",
              "1048571        NaN  responded  VBD   O\n",
              "1048572        NaN         to   TO   O\n",
              "1048573        NaN        the   DT   O\n",
              "1048574        NaN     attack   NN   O"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "sVEnvXJOFdgX",
        "outputId": "df751d46-8cb4-4016-fe6c-049eb990eea7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence #</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>47959</td>\n",
              "      <td>1048575</td>\n",
              "      <td>1048575</td>\n",
              "      <td>1048575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>47959</td>\n",
              "      <td>35178</td>\n",
              "      <td>42</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>Sentence: 46513</td>\n",
              "      <td>the</td>\n",
              "      <td>NN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>1</td>\n",
              "      <td>52573</td>\n",
              "      <td>145807</td>\n",
              "      <td>887908</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Sentence #     Word      POS      Tag\n",
              "count             47959  1048575  1048575  1048575\n",
              "unique            47959    35178       42       17\n",
              "top     Sentence: 46513      the       NN        O\n",
              "freq                  1    52573   145807   887908"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnpBJn7AFfZ2",
        "outputId": "96931236-ee99-4fe6-b1c7-62e28e4302d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sentence #    1000616\n",
              "Word                0\n",
              "POS                 0\n",
              "Tag                 0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# see if there is missing value in data\n",
        "\n",
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DSa1gREFhh0",
        "outputId": "34de38f2-2bc9-42b4-a0c5-85cda3247c39"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sentence #      47959\n",
              "Word          1048575\n",
              "POS           1048575\n",
              "Tag           1048575\n",
              "dtype: int64"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Count of each\n",
        "data.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlDmp4DVF_ub"
      },
      "source": [
        "### **2. Data Exploration:**\n",
        "\n",
        "### **a. Number of Sentences**\n",
        "### **b. Distribution of Named Entity Tags**\n",
        "### **c. Average Words per Sentence**\n",
        "### **d. Most Frequent Part-of-Speech Tags**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydXEhpAfGUiu",
        "outputId": "b0130f85-7b41-44e7-fec6-cb7b22f9315f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sentences: 47959\n",
            "Distribution of named entity tags:\n",
            "O        887908\n",
            "B-geo     37644\n",
            "B-tim     20333\n",
            "B-org     20143\n",
            "I-per     17251\n",
            "B-per     16990\n",
            "I-org     16784\n",
            "B-gpe     15870\n",
            "I-geo      7414\n",
            "I-tim      6528\n",
            "B-art       402\n",
            "B-eve       308\n",
            "I-art       297\n",
            "I-eve       253\n",
            "B-nat       201\n",
            "I-gpe       198\n",
            "I-nat        51\n",
            "Name: Tag, dtype: int64\n",
            "Average words per sentence: 1.0\n",
            "Most frequent part-of-speech tags:\n",
            "NN     145807\n",
            "NNP    131426\n",
            "IN     120996\n",
            "DT      98454\n",
            "JJ      78412\n",
            "NNS     75840\n",
            ".       47831\n",
            "VBD     39379\n",
            ",       32757\n",
            "VBN     32328\n",
            "Name: POS, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# a. Number of Sentences:\n",
        "\n",
        "sentence_count = data['Sentence #'].nunique()\n",
        "print(f\"Number of sentences: {sentence_count}\")\n",
        "\n",
        "# b. Distribution of Named Entity Tags:\n",
        "\n",
        "tag_counts = data['Tag'].value_counts()\n",
        "print(f\"Distribution of named entity tags:\\n{tag_counts}\")\n",
        "\n",
        "# c. Average Words per Sentence:**\n",
        "\n",
        "avg_words_per_sentence = data['Sentence #'].value_counts().mean()\n",
        "print(f\"Average words per sentence: {avg_words_per_sentence}\")\n",
        "\n",
        "# d. Most Frequent Part-of-Speech Tags:**\n",
        "\n",
        "pos_counts = data['POS'].value_counts().head(10)\n",
        "print(f\"Most frequent part-of-speech tags:\\n{pos_counts}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rG55E1J_Gh1D"
      },
      "source": [
        "### **3. Visualization:**\n",
        "\n",
        "### **a. Distribution of Named Entity Tags (Pie Chart):**\n",
        "### **b. Part-of-Speech Tag Distribution (Bar Chart):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 924
        },
        "id": "d0jZnp4BGVgf",
        "outputId": "afa518de-8be3-421f-c910-ee44941641be"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# a. Distribution of Named Entity Tags (Pie Chart):\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.pie(tag_counts, labels=tag_counts.index, autopct=\"%1.1f%%\")\n",
        "plt.title(\"Distribution of Named Entity Tags\")\n",
        "plt.show()\n",
        "\n",
        "# b. Part-of-Speech Tag Distribution (Bar Chart):\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "pos_counts.plot(kind='bar')\n",
        "plt.title(\"Part-of-Speech Tag Distribution\")\n",
        "plt.xlabel(\"POS Tag\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1f3dKvlGne2"
      },
      "source": [
        "### **4. Summary:**\n",
        "\n",
        "### The EDA reveals the following insights:\n",
        "\n",
        "### - The dataset contains a significant number of sentences and named entities.\n",
        "### - The distribution of named entity tags is imbalanced, with some tags being more frequent than others.\n",
        "### - The average sentence length is around 15 words.\n",
        "### - The most frequent part-of-speech tags are nouns, verbs, and adjectives.\n",
        "\n",
        "### These findings can be used to guide further data processing and model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1II7ekZGsml"
      },
      "source": [
        "### Lets look at each tag's distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4Jn1fVT_GkO",
        "outputId": "284fdadd-e089-43b7-97fe-610f4a1e2d4d",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(O, 887908)\n",
            "(B-geo, 37644)\n",
            "(B-gpe, 15870)\n",
            "(B-per, 16990)\n",
            "(I-geo, 7414)\n",
            "(B-org, 20143)\n",
            "(I-org, 16784)\n",
            "(B-tim, 20333)\n",
            "(B-art, 402)\n",
            "(I-art, 297)\n",
            "(I-per, 17251)\n",
            "(I-gpe, 198)\n",
            "(I-tim, 6528)\n",
            "(B-nat, 201)\n",
            "(B-eve, 308)\n",
            "(I-eve, 253)\n",
            "(I-nat, 51)\n"
          ]
        }
      ],
      "source": [
        "tag_distribution = {}\n",
        "for tag in data['Tag'].unique():\n",
        "  count = len(data[data['Tag'] == tag])\n",
        "  tag_distribution[tag] = count\n",
        "\n",
        "for tag, count in tag_distribution.items():\n",
        "  print(f\"({tag}, {count})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsX0ebih_GkS"
      },
      "source": [
        "### It can be seen that nat, eve and art have very little represenation in the data set.  We can eliminate them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8iorLrU4_GkW"
      },
      "outputs": [],
      "source": [
        "#  remove all entities with nat,  eve and art from data set\n",
        "\n",
        "data = data[~data['Tag'].isin([\"B-art\", \"I-art\", \"B-eve\", \"I-eve\", \"B-nat\", \"I-nat\"])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNt3rLDfG8ZX",
        "outputId": "541e1832-e5b0-4fdb-992e-a1fc3f40ab48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(O, 887908)\n",
            "(B-geo, 37644)\n",
            "(B-gpe, 15870)\n",
            "(B-per, 16990)\n",
            "(I-geo, 7414)\n",
            "(B-org, 20143)\n",
            "(I-org, 16784)\n",
            "(B-tim, 20333)\n",
            "(I-per, 17251)\n",
            "(I-gpe, 198)\n",
            "(I-tim, 6528)\n"
          ]
        }
      ],
      "source": [
        "## Checking our removal operation\n",
        "tag_distribution = {}\n",
        "for tag in data['Tag'].unique():\n",
        "  count = len(data[data['Tag'] == tag])\n",
        "  tag_distribution[tag] = count\n",
        "\n",
        "for tag, count in tag_distribution.items():\n",
        "  print(f\"({tag}, {count})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mskU4h0oRKEF"
      },
      "source": [
        "### filling missing values in sentence column based on the last upper sentence that was not missing value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "zkW2vNcO-uMH",
        "outputId": "51efa011-bac9-4564-ac7f-5f53b94dcca3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence #</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>of</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>have</td>\n",
              "      <td>VBP</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>marched</td>\n",
              "      <td>VBN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Sentence #           Word  POS Tag\n",
              "0  Sentence: 1      Thousands  NNS   O\n",
              "1  Sentence: 1             of   IN   O\n",
              "2  Sentence: 1  demonstrators  NNS   O\n",
              "3  Sentence: 1           have  VBP   O\n",
              "4  Sentence: 1        marched  VBN   O"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = data.fillna(method='ffill')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgOHiM4tHLmp"
      },
      "source": [
        "### **Adding New Aggrigated Columns**\n",
        "\n",
        "### a. group the words by sentence\n",
        "### b. group the tags by sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "M-cuhR_IHLG5"
      },
      "outputs": [],
      "source": [
        "# a group the words by sentence\n",
        "data['sentence'] = data[['Sentence #','Word','Tag']].groupby(['Sentence #'])['Word'].transform(lambda x: ' '.join(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "Hmd-ow389k6Y",
        "outputId": "77667e34-8e90-46e7-e183-3932bb76debf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence #</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "      <th>sentence</th>\n",
              "      <th>word_labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "      <td>Thousands of demonstrators have marched throug...</td>\n",
              "      <td>O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>of</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "      <td>Thousands of demonstrators have marched throug...</td>\n",
              "      <td>O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "      <td>Thousands of demonstrators have marched throug...</td>\n",
              "      <td>O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>have</td>\n",
              "      <td>VBP</td>\n",
              "      <td>O</td>\n",
              "      <td>Thousands of demonstrators have marched throug...</td>\n",
              "      <td>O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>marched</td>\n",
              "      <td>VBN</td>\n",
              "      <td>O</td>\n",
              "      <td>Thousands of demonstrators have marched throug...</td>\n",
              "      <td>O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Sentence #           Word  POS Tag  \\\n",
              "0  Sentence: 1      Thousands  NNS   O   \n",
              "1  Sentence: 1             of   IN   O   \n",
              "2  Sentence: 1  demonstrators  NNS   O   \n",
              "3  Sentence: 1           have  VBP   O   \n",
              "4  Sentence: 1        marched  VBN   O   \n",
              "\n",
              "                                            sentence  \\\n",
              "0  Thousands of demonstrators have marched throug...   \n",
              "1  Thousands of demonstrators have marched throug...   \n",
              "2  Thousands of demonstrators have marched throug...   \n",
              "3  Thousands of demonstrators have marched throug...   \n",
              "4  Thousands of demonstrators have marched throug...   \n",
              "\n",
              "                                         word_labels  \n",
              "0  O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...  \n",
              "1  O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...  \n",
              "2  O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...  \n",
              "3  O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...  \n",
              "4  O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...  "
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# b  group the tags by sentence\n",
        "data['word_labels'] = data[['Sentence #','Word','Tag']].groupby(['Sentence #'])['Tag'].transform(lambda x: ','.join(x))\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsjhdQbE-Lve"
      },
      "source": [
        "### Create dictionaries to map tags to numbers, and numbers to tag. This is so that we can have an interger to feed our model with and when our model makes a prediction we can do the reverse map and have the integer converted back to our tag.  We are essentianlly encoding our tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFRDM8WsQXvL",
        "outputId": "502b67c4-1cdb-4fd4-f077-95b070482358"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'O': 0,\n",
              " 'B-geo': 1,\n",
              " 'B-gpe': 2,\n",
              " 'B-per': 3,\n",
              " 'I-geo': 4,\n",
              " 'B-org': 5,\n",
              " 'I-org': 6,\n",
              " 'B-tim': 7,\n",
              " 'I-per': 8,\n",
              " 'I-gpe': 9,\n",
              " 'I-tim': 10}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label2id = {k: v for v, k in enumerate(data.Tag.unique())}\n",
        "id2label = {v: k for v, k in enumerate(data.Tag.unique())}\n",
        "label2id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J08Cvk_USgbM"
      },
      "source": [
        "### we can now trim the data for our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "SrEgd4PZUgmF",
        "outputId": "1cf27cb2-ccb8-4e80-b61d-1c7865c48582"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>word_labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Thousands of demonstrators have marched throug...</td>\n",
              "      <td>O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Families of soldiers killed in the conflict jo...</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-per,O,O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>They marched from the Houses of Parliament to ...</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,B-geo,I-geo,O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Police put the number of marchers at 10,000 wh...</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The protest comes on the eve of the annual con...</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,B-geo,O,O,B-org,I-org,O,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  \\\n",
              "0  Thousands of demonstrators have marched throug...   \n",
              "1  Families of soldiers killed in the conflict jo...   \n",
              "2  They marched from the Houses of Parliament to ...   \n",
              "3  Police put the number of marchers at 10,000 wh...   \n",
              "4  The protest comes on the eve of the annual con...   \n",
              "\n",
              "                                         word_labels  \n",
              "0  O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...  \n",
              "1  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-per,O,O,...  \n",
              "2                O,O,O,O,O,O,O,O,O,O,O,B-geo,I-geo,O  \n",
              "3                      O,O,O,O,O,O,O,O,O,O,O,O,O,O,O  \n",
              "4  O,O,O,O,O,O,O,O,O,O,O,B-geo,O,O,B-org,I-org,O,...  "
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = data[[\"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3ArUiVRqw0C",
        "outputId": "156be32a-fe02-41ff-d976-f9b87eefd3f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "47571"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8obZumRTBrT"
      },
      "source": [
        "### Testing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "eUvupomW_fbe",
        "outputId": "e22829c9-0c60-4922-ee89-e37be857c6ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Local news reports said at least five mortar shells hit the palace compound and other mortars were fired elsewhere in Mogadishu Wednesday .'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.iloc[20].sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0dLyY3Oi_lvp",
        "outputId": "43531a42-220b-4f99-f41b-073610a13eec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-geo,B-tim,O'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.iloc[20].word_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5EHpuB78pIa"
      },
      "source": [
        "## Preparing Dataloaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyGRy7VrH5g3"
      },
      "source": [
        "### This function was adopted from [here](https://github.com/chambliss/Multilingual_NER/blob/8d3afffd4c99774e0585f4c7d721bb99481fd60f/python/utils/main_utils.py#L118)\n",
        "#### Its job is to define the labels at the wordpiece-level, rather than the word-level.  An example would be the word icecream we want the model to understand ice+cream not icecream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "RNzSgZTfGUd8"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
        "    \"\"\"\n",
        "    Word piece tokenization makes it difficult to match word labels\n",
        "    back up with individual word pieces. This function tokenizes each\n",
        "    word one at a time so that it is easier to preserve the correct\n",
        "    label for each subword. It is, of course, a bit slower in processing\n",
        "    time, but it will help our model achieve higher accuracy.\n",
        "    \"\"\"\n",
        "\n",
        "    tokenized_sentence = []\n",
        "    labels = []\n",
        "\n",
        "    sentence = sentence.strip()\n",
        "\n",
        "    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n",
        "\n",
        "        # Tokenize the word and count # of subwords the word is broken into\n",
        "        tokenized_word = tokenizer.tokenize(word)\n",
        "        n_subwords = len(tokenized_word)\n",
        "\n",
        "        # Add the tokenized word to the final tokenized word list\n",
        "        tokenized_sentence.extend(tokenized_word)\n",
        "\n",
        "        # Add the same label to the new list of labels `n_subwords` times\n",
        "        labels.extend([label] * n_subwords)\n",
        "\n",
        "    return tokenized_sentence, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ez7qlFHl56ZW"
      },
      "source": [
        "### Creating a torch data loader. Bert needs equal lenghth imput so we will add or trim based on our decided max length  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "aJty_Abw8_xK"
      },
      "outputs": [],
      "source": [
        "class dataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # step 1: tokenize (and adapt corresponding labels)\n",
        "        sentence = self.data.sentence[index]\n",
        "        word_labels = self.data.word_labels[index]\n",
        "        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n",
        "\n",
        "        # step 2: add special tokens (and corresponding labels)\n",
        "        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n",
        "        labels.insert(0, \"O\") # add outside label for [CLS] token\n",
        "        labels.insert(-1, \"O\") # add outside label for [SEP] token\n",
        "\n",
        "        # step 3: truncating/padding\n",
        "        maxlen = self.max_len\n",
        "\n",
        "        if (len(tokenized_sentence) > maxlen):\n",
        "          # truncate\n",
        "          tokenized_sentence = tokenized_sentence[:maxlen]\n",
        "          labels = labels[:maxlen]\n",
        "        else:\n",
        "          # pad\n",
        "          tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n",
        "          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n",
        "\n",
        "        # step 4: obtain the attention mask\n",
        "        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n",
        "\n",
        "        # step 5: convert tokens to input ids\n",
        "        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
        "\n",
        "        label_ids = [label2id[label] for label in labels]\n",
        "        # the following line is deprecated\n",
        "        #label_ids = [label if label != 0 else -100 for label in label_ids]\n",
        "\n",
        "        return {\n",
        "              'ids': torch.tensor(ids, dtype=torch.long),\n",
        "              'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
        "              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n",
        "              'targets': torch.tensor(label_ids, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoPNgmWKImRd"
      },
      "source": [
        "### Define training params such as batch size epoch and bert tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "lgNSM8Xz79Mg",
        "tags": []
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 4\n",
        "VALID_BATCH_SIZE = 2\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 1e-05\n",
        "MAX_GRAD_NORM = 10\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTP7zuWGWGUd"
      },
      "source": [
        "### Now, based on the class we defined above, we can create 2 datasets, one for training and one for testing. Let's use a 80/20 split:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrkdZBLYHVcB",
        "outputId": "46782ba4-6d62-4a37-c105-bbb29867db09",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All Dataset: 47571\n",
            "Train Data: 38057\n",
            "TEST Data: 9514\n"
          ]
        }
      ],
      "source": [
        "train_size = 0.8\n",
        "train_dataset = data.sample(frac=train_size,random_state=200)\n",
        "test_dataset = data.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "print(\"All Dataset: {}\".format(data.shape[0]))\n",
        "print(\"Train Data: {}\".format(train_dataset.shape[0]))\n",
        "print(\"TEST Data: {}\".format(test_dataset.shape[0]))\n",
        "\n",
        "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ptv5AT_iTb7W"
      },
      "source": [
        "### Inspection of our training and test data after tokenizition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phmPylgAm8Xy",
        "outputId": "65221ff8-de29-4a30-e6aa-c0a171dd7e4b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ids': tensor([  101,  2009,  2758,  1996, 10284,  2097,  6848,  2129,  2000,  7496,\n",
              "          1996, 10859,  1997,  3032,  9936,  2312,  3616,  1997,  8956,  8711,\n",
              "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
              " 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'targets': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0])}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_set[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHoufyakY18x",
        "outputId": "6130ebdc-85a3-451a-b55a-b203808b80cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([  101,  2009,  2758,  1996, 10284,  2097,  6848,  2129,  2000,  7496,\n",
              "         1996, 10859,  1997,  3032,  9936,  2312,  3616,  1997,  8956,  8711,\n",
              "         1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_set[10][\"ids\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky68FcTgWnfN"
      },
      "source": [
        "### Make torch data loaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "KIw793myWOmi"
      },
      "outputs": [],
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73OzU7oXRxR8"
      },
      "source": [
        "## **Model Definition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cB9MR3KcWXUs",
        "outputId": "d9dbefe5-ad3f-4e90-f646-a12d3ce6f873",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=11, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = BertForTokenClassification.from_pretrained('bert-base-uncased',\n",
        "                                                   num_labels=len(id2label),\n",
        "                                                   id2label=id2label,\n",
        "                                                   label2id=label2id)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp7Yl4JyWhDj"
      },
      "source": [
        "## **Training and Evaluation**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzrPkRKlJEc1"
      },
      "source": [
        "### Defining The optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "kznSQfGIWdU4"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZQ8JMF0NOe1"
      },
      "source": [
        "###  PyTorch training function.adopted from this [repository](https://github.com/chambliss/Multilingual_NER/blob/master/python/utils/main_utils.py#L344)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "GLFivpkwW1HY"
      },
      "outputs": [],
      "source": [
        "# Defining the training function on the 80% of the dataset for tuning the bert model\n",
        "def train(epoch):\n",
        "    tr_loss, tr_accuracy = 0, 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    tr_preds, tr_labels = [], []\n",
        "    # put model in training mode\n",
        "    model.train()\n",
        "\n",
        "    for idx, batch in enumerate(training_loader):\n",
        "\n",
        "        ids = batch['ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['mask'].to(device, dtype = torch.long)\n",
        "        targets = batch['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "        loss, tr_logits = outputs.loss, outputs.logits\n",
        "        tr_loss += loss.item()\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples += targets.size(0)\n",
        "\n",
        "        if idx % 100==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
        "\n",
        "        # compute training accuracy\n",
        "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
        "        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
        "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "\n",
        "        tr_preds.extend(predictions)\n",
        "        tr_labels.extend(targets)\n",
        "\n",
        "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "        tr_accuracy += tmp_tr_accuracy\n",
        "\n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
        "        )\n",
        "\n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
        "    print(f\"Training loss epoch: {epoch_loss}\")\n",
        "    print(f\"Training accuracy epoch: {tr_accuracy}\")\n",
        "\n",
        "    return{\"Training loss\": epoch_loss, \"Training accuracy\": tr_accuracy}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "RIVVfFHi7Aw7"
      },
      "outputs": [],
      "source": [
        "# Validation Function\n",
        "def valid(model, testing_loader):\n",
        "    # put model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_examples, nb_eval_steps = 0, 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(testing_loader):\n",
        "\n",
        "            ids = batch['ids'].to(device, dtype = torch.long)\n",
        "            mask = batch['mask'].to(device, dtype = torch.long)\n",
        "            targets = batch['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "            loss, eval_logits = outputs.loss, outputs.logits\n",
        "\n",
        "            eval_loss += loss.item()\n",
        "\n",
        "            nb_eval_steps += 1\n",
        "            nb_eval_examples += targets.size(0)\n",
        "\n",
        "            if idx % 100==0:\n",
        "                loss_step = eval_loss/nb_eval_steps\n",
        "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
        "\n",
        "            # compute evaluation accuracy\n",
        "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
        "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "\n",
        "            eval_labels.extend(targets)\n",
        "            eval_preds.extend(predictions)\n",
        "\n",
        "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "\n",
        "    labels = [id2label[id.item()] for id in eval_labels]\n",
        "    predictions = [id2label[id.item()] for id in eval_preds]\n",
        "\n",
        "\n",
        "\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
        "    print(f\"Validation Loss: {eval_loss}\")\n",
        "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
        "\n",
        "    return labels, predictions, {\"Validation loss epoch\": eval_loss, \"Validation accuracy epoch\": eval_accuracy}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y07Ybw8rZeZ7",
        "outputId": "bd98db68-1fb1-4e49-f56d-7aeda962c3ca",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training epoch: 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss per 100 training steps: 2.433394432067871\n",
            "Training loss per 100 training steps: 0.435930235329831\n",
            "Training loss per 100 training steps: 0.2901637788621051\n",
            "Training loss per 100 training steps: 0.22497705690117176\n",
            "Training loss per 100 training steps: 0.1879133591033574\n",
            "Training loss per 100 training steps: 0.16300891254454136\n",
            "Training loss per 100 training steps: 0.1441797023250128\n",
            "Training loss per 100 training steps: 0.1317745158003747\n",
            "Training loss per 100 training steps: 0.12218645687323161\n",
            "Training loss per 100 training steps: 0.11377676475801392\n",
            "Training loss per 100 training steps: 0.10672107624442695\n",
            "Training loss per 100 training steps: 0.10132012609180932\n",
            "Training loss per 100 training steps: 0.09608046288643153\n",
            "Training loss per 100 training steps: 0.09170145537456545\n",
            "Training loss per 100 training steps: 0.08793908410950256\n",
            "Training loss per 100 training steps: 0.08479333270063809\n",
            "Training loss per 100 training steps: 0.08185244617919128\n",
            "Training loss per 100 training steps: 0.07902326404066676\n",
            "Training loss per 100 training steps: 0.07664992494018917\n",
            "Training loss per 100 training steps: 0.07463355484133932\n",
            "Training loss per 100 training steps: 0.07280936120226905\n",
            "Training loss per 100 training steps: 0.07126928293928575\n",
            "Training loss per 100 training steps: 0.06958409776887443\n",
            "Training loss per 100 training steps: 0.06803302956127384\n",
            "Training loss per 100 training steps: 0.0666461163705664\n",
            "Training loss per 100 training steps: 0.06521142038056997\n",
            "Training loss per 100 training steps: 0.06415709044866841\n",
            "Training loss per 100 training steps: 0.06300755832224474\n",
            "Training loss per 100 training steps: 0.061918249830940415\n",
            "Training loss per 100 training steps: 0.06100012855847475\n",
            "Training loss per 100 training steps: 0.05992905065947656\n",
            "Training loss per 100 training steps: 0.05922312200789018\n",
            "Training loss per 100 training steps: 0.058349981477682826\n",
            "Training loss per 100 training steps: 0.05760738883967777\n",
            "Training loss per 100 training steps: 0.056882154498030496\n",
            "Training loss per 100 training steps: 0.05621275895522437\n",
            "Training loss per 100 training steps: 0.05566632185969657\n",
            "Training loss per 100 training steps: 0.055115063585604196\n",
            "Training loss per 100 training steps: 0.05449634249597703\n",
            "Training loss per 100 training steps: 0.05390682161343218\n",
            "Training loss per 100 training steps: 0.05339368460610735\n",
            "Training loss per 100 training steps: 0.05280261021373564\n",
            "Training loss per 100 training steps: 0.05225061741981085\n",
            "Training loss per 100 training steps: 0.05186716388360114\n",
            "Training loss per 100 training steps: 0.051404039021813924\n",
            "Training loss per 100 training steps: 0.05095128860286523\n",
            "Training loss per 100 training steps: 0.05045268424427513\n",
            "Training loss per 100 training steps: 0.05002361250333614\n",
            "Training loss per 100 training steps: 0.04956229039269109\n",
            "Training loss per 100 training steps: 0.049216882416678376\n",
            "Training loss per 100 training steps: 0.04875694054515476\n",
            "Training loss per 100 training steps: 0.0483690128085652\n",
            "Training loss per 100 training steps: 0.048044886463826486\n",
            "Training loss per 100 training steps: 0.0476863979753637\n",
            "Training loss per 100 training steps: 0.047287821679503794\n",
            "Training loss per 100 training steps: 0.046984740359988814\n",
            "Training loss per 100 training steps: 0.04669482015083422\n",
            "Training loss per 100 training steps: 0.046309007668023414\n",
            "Training loss per 100 training steps: 0.04599318383512313\n",
            "Training loss per 100 training steps: 0.04569867365555735\n",
            "Training loss per 100 training steps: 0.04540683731906561\n",
            "Training loss per 100 training steps: 0.045123475689153106\n",
            "Training loss per 100 training steps: 0.044839963874273844\n",
            "Training loss per 100 training steps: 0.044561529380745174\n",
            "Training loss per 100 training steps: 0.04440410239839423\n",
            "Training loss per 100 training steps: 0.04411750933100317\n",
            "Training loss per 100 training steps: 0.04389269450118367\n",
            "Training loss per 100 training steps: 0.04370430161749865\n",
            "Training loss per 100 training steps: 0.043478688053484156\n",
            "Training loss per 100 training steps: 0.043314038247821374\n",
            "Training loss per 100 training steps: 0.04312304410167119\n",
            "Training loss per 100 training steps: 0.04293336583184502\n",
            "Training loss per 100 training steps: 0.04272280050407802\n",
            "Training loss per 100 training steps: 0.04249212981194916\n",
            "Training loss per 100 training steps: 0.04231188739843112\n",
            "Training loss per 100 training steps: 0.042086696419951185\n",
            "Training loss per 100 training steps: 0.04186919740396813\n",
            "Training loss per 100 training steps: 0.041661180941728715\n",
            "Training loss per 100 training steps: 0.041457224206577305\n",
            "Training loss per 100 training steps: 0.04128909956645591\n",
            "Training loss per 100 training steps: 0.04111364486753881\n",
            "Training loss per 100 training steps: 0.04097527514238979\n",
            "Training loss per 100 training steps: 0.04086659508424344\n",
            "Training loss per 100 training steps: 0.04072479374167779\n",
            "Training loss per 100 training steps: 0.040541620559128286\n",
            "Training loss per 100 training steps: 0.04037293484874963\n",
            "Training loss per 100 training steps: 0.04023365423396656\n",
            "Training loss per 100 training steps: 0.040064629850387015\n",
            "Training loss per 100 training steps: 0.03991481658516478\n",
            "Training loss per 100 training steps: 0.03975971260809227\n",
            "Training loss per 100 training steps: 0.039631981277318706\n",
            "Training loss per 100 training steps: 0.039474881417800436\n",
            "Training loss per 100 training steps: 0.03932957929432616\n",
            "Training loss per 100 training steps: 0.03923766373848454\n",
            "Training loss per 100 training steps: 0.03914158968794897\n",
            "Training loss per 100 training steps: 0.038992878034017096\n",
            "Training loss epoch: 0.03897308698877425\n",
            "Training accuracy epoch: 0.9495700795696844\n",
            "Validation loss per 100 evaluation steps: 0.002820770489051938\n",
            "Validation loss per 100 evaluation steps: 0.02839915755770796\n",
            "Validation loss per 100 evaluation steps: 0.026627642109869794\n",
            "Validation loss per 100 evaluation steps: 0.026465593851403006\n",
            "Validation loss per 100 evaluation steps: 0.026092775048521633\n",
            "Validation loss per 100 evaluation steps: 0.025458379329936002\n",
            "Validation loss per 100 evaluation steps: 0.024908476590254743\n",
            "Validation loss per 100 evaluation steps: 0.02439847054145985\n",
            "Validation loss per 100 evaluation steps: 0.02457252623411223\n",
            "Validation loss per 100 evaluation steps: 0.024361564659353286\n",
            "Validation loss per 100 evaluation steps: 0.02420438372853398\n",
            "Validation loss per 100 evaluation steps: 0.024140993058336127\n",
            "Validation loss per 100 evaluation steps: 0.02441739992958701\n",
            "Validation loss per 100 evaluation steps: 0.02447082156937487\n",
            "Validation loss per 100 evaluation steps: 0.024103404115359404\n",
            "Validation loss per 100 evaluation steps: 0.024133217683373297\n",
            "Validation loss per 100 evaluation steps: 0.02405761343499744\n",
            "Validation loss per 100 evaluation steps: 0.024181473300549438\n",
            "Validation loss per 100 evaluation steps: 0.023990159402409846\n",
            "Validation loss per 100 evaluation steps: 0.024147423113259597\n",
            "Validation loss per 100 evaluation steps: 0.024233043943031694\n",
            "Validation loss per 100 evaluation steps: 0.024349076403379806\n",
            "Validation loss per 100 evaluation steps: 0.024304936976342912\n",
            "Validation loss per 100 evaluation steps: 0.02443534516406243\n",
            "Validation loss per 100 evaluation steps: 0.02433463764718651\n",
            "Validation loss per 100 evaluation steps: 0.02424202795128756\n",
            "Validation loss per 100 evaluation steps: 0.02440694541005162\n",
            "Validation loss per 100 evaluation steps: 0.024501148609052434\n",
            "Validation loss per 100 evaluation steps: 0.024419752944907894\n",
            "Validation loss per 100 evaluation steps: 0.024582518098809125\n",
            "Validation loss per 100 evaluation steps: 0.024646120554051854\n",
            "Validation loss per 100 evaluation steps: 0.024617887600664138\n",
            "Validation loss per 100 evaluation steps: 0.024513681078417077\n",
            "Validation loss per 100 evaluation steps: 0.0245651963292583\n",
            "Validation loss per 100 evaluation steps: 0.02457438592684518\n",
            "Validation loss per 100 evaluation steps: 0.024610271437033395\n",
            "Validation loss per 100 evaluation steps: 0.024663657581594924\n",
            "Validation loss per 100 evaluation steps: 0.024704679836672423\n",
            "Validation loss per 100 evaluation steps: 0.024743908093056776\n",
            "Validation loss per 100 evaluation steps: 0.024675550898958863\n",
            "Validation loss per 100 evaluation steps: 0.024558060849468433\n",
            "Validation loss per 100 evaluation steps: 0.024485322907929243\n",
            "Validation loss per 100 evaluation steps: 0.0245524062706767\n",
            "Validation loss per 100 evaluation steps: 0.024577765202863196\n",
            "Validation loss per 100 evaluation steps: 0.02465861543461803\n",
            "Validation loss per 100 evaluation steps: 0.0247392631768296\n",
            "Validation loss per 100 evaluation steps: 0.02482417787449915\n",
            "Validation loss per 100 evaluation steps: 0.024839475913171902\n",
            "Validation Loss: 0.024871907058671323\n",
            "Validation Accuracy: 0.9633732638163968\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.008959604427218437\n",
            "Training loss per 100 training steps: 0.022576246634794232\n",
            "Training loss per 100 training steps: 0.023143582563240322\n",
            "Training loss per 100 training steps: 0.02241621514669375\n",
            "Training loss per 100 training steps: 0.02243730460033471\n",
            "Training loss per 100 training steps: 0.022165840264282442\n",
            "Training loss per 100 training steps: 0.023023921356651895\n",
            "Training loss per 100 training steps: 0.022935865357828416\n",
            "Training loss per 100 training steps: 0.02267756690229051\n",
            "Training loss per 100 training steps: 0.023122968008293648\n",
            "Training loss per 100 training steps: 0.022913456487934105\n",
            "Training loss per 100 training steps: 0.022816208725235595\n",
            "Training loss per 100 training steps: 0.022890186254253035\n",
            "Training loss per 100 training steps: 0.02280107902884709\n",
            "Training loss per 100 training steps: 0.022604545994310592\n",
            "Training loss per 100 training steps: 0.022538427978199965\n",
            "Training loss per 100 training steps: 0.022706711600516585\n",
            "Training loss per 100 training steps: 0.022693221705326748\n",
            "Training loss per 100 training steps: 0.022751793249247535\n",
            "Training loss per 100 training steps: 0.022810729360563102\n",
            "Training loss per 100 training steps: 0.02286233575986712\n",
            "Training loss per 100 training steps: 0.022751452875640978\n",
            "Training loss per 100 training steps: 0.02288422823773789\n",
            "Training loss per 100 training steps: 0.022996014486722838\n",
            "Training loss per 100 training steps: 0.02290328158724628\n",
            "Training loss per 100 training steps: 0.02288768773833246\n",
            "Training loss per 100 training steps: 0.02296237873142931\n",
            "Training loss per 100 training steps: 0.023128893050840853\n",
            "Training loss per 100 training steps: 0.02301490716151671\n",
            "Training loss per 100 training steps: 0.02298061311566421\n",
            "Training loss per 100 training steps: 0.02302862623743332\n",
            "Training loss per 100 training steps: 0.023072314570452446\n",
            "Training loss per 100 training steps: 0.023004020304546645\n",
            "Training loss per 100 training steps: 0.02289075927080324\n",
            "Training loss per 100 training steps: 0.022871012606741254\n",
            "Training loss per 100 training steps: 0.022836264033502553\n",
            "Training loss per 100 training steps: 0.022788745136889995\n",
            "Training loss per 100 training steps: 0.022794178892166533\n",
            "Training loss per 100 training steps: 0.0226962566332739\n",
            "Training loss per 100 training steps: 0.022670740494642592\n",
            "Training loss per 100 training steps: 0.02266255491818273\n",
            "Training loss per 100 training steps: 0.02267980171946182\n",
            "Training loss per 100 training steps: 0.022678930383325725\n",
            "Training loss per 100 training steps: 0.02266734623645518\n",
            "Training loss per 100 training steps: 0.02266894236159302\n",
            "Training loss per 100 training steps: 0.022604207256577004\n",
            "Training loss per 100 training steps: 0.022674932122895144\n",
            "Training loss per 100 training steps: 0.022640732645308224\n",
            "Training loss per 100 training steps: 0.022644454860583235\n",
            "Training loss per 100 training steps: 0.022634575615373794\n",
            "Training loss per 100 training steps: 0.02264928730612993\n",
            "Training loss per 100 training steps: 0.022572731515429063\n",
            "Training loss per 100 training steps: 0.022645061232146445\n",
            "Training loss per 100 training steps: 0.022640456545226385\n",
            "Training loss per 100 training steps: 0.022659092216777667\n",
            "Training loss per 100 training steps: 0.022679119599795957\n",
            "Training loss per 100 training steps: 0.02264482033926601\n",
            "Training loss per 100 training steps: 0.022709618435068874\n",
            "Training loss per 100 training steps: 0.02268803963976797\n",
            "Training loss per 100 training steps: 0.022666450953678332\n",
            "Training loss per 100 training steps: 0.022602858970596093\n",
            "Training loss per 100 training steps: 0.022619157575083815\n",
            "Training loss per 100 training steps: 0.022615326867808826\n",
            "Training loss per 100 training steps: 0.022593997827504986\n",
            "Training loss per 100 training steps: 0.022616751228154347\n",
            "Training loss per 100 training steps: 0.022620429513043133\n",
            "Training loss per 100 training steps: 0.022570809427613563\n",
            "Training loss per 100 training steps: 0.022573935465197297\n",
            "Training loss per 100 training steps: 0.022565960687114882\n",
            "Training loss per 100 training steps: 0.022598196710785425\n",
            "Training loss per 100 training steps: 0.022647359373334647\n",
            "Training loss per 100 training steps: 0.022666594017702057\n",
            "Training loss per 100 training steps: 0.022638201986112858\n",
            "Training loss per 100 training steps: 0.02261295029049124\n",
            "Training loss per 100 training steps: 0.022616861515512385\n",
            "Training loss per 100 training steps: 0.022621184223829166\n",
            "Training loss per 100 training steps: 0.022614009246610715\n",
            "Training loss per 100 training steps: 0.02255618273572495\n",
            "Training loss per 100 training steps: 0.022524297739998797\n",
            "Training loss per 100 training steps: 0.022463181447942432\n",
            "Training loss per 100 training steps: 0.02245173615716879\n",
            "Training loss per 100 training steps: 0.02242326051416855\n",
            "Training loss per 100 training steps: 0.022388375817721302\n",
            "Training loss per 100 training steps: 0.022359304319666852\n",
            "Training loss per 100 training steps: 0.022402243245943112\n",
            "Training loss per 100 training steps: 0.022396347094593188\n",
            "Training loss per 100 training steps: 0.022357452296394603\n",
            "Training loss per 100 training steps: 0.022362660865556647\n",
            "Training loss per 100 training steps: 0.02235037793286971\n",
            "Training loss per 100 training steps: 0.022326320044535768\n",
            "Training loss per 100 training steps: 0.022324413996269447\n",
            "Training loss per 100 training steps: 0.022329431567885336\n",
            "Training loss per 100 training steps: 0.02230184851607386\n",
            "Training loss per 100 training steps: 0.0222880181063949\n",
            "Training loss per 100 training steps: 0.02226153506649139\n",
            "Training loss per 100 training steps: 0.022262092032524917\n",
            "Training loss epoch: 0.02227375777897628\n",
            "Training accuracy epoch: 0.9657434621740814\n",
            "Validation loss per 100 evaluation steps: 0.010380572639405727\n",
            "Validation loss per 100 evaluation steps: 0.023152003880811504\n",
            "Validation loss per 100 evaluation steps: 0.02374177820282926\n",
            "Validation loss per 100 evaluation steps: 0.02685576171822211\n",
            "Validation loss per 100 evaluation steps: 0.026929817287261296\n",
            "Validation loss per 100 evaluation steps: 0.02673672960682705\n",
            "Validation loss per 100 evaluation steps: 0.02586007330494805\n",
            "Validation loss per 100 evaluation steps: 0.02529314097436938\n",
            "Validation loss per 100 evaluation steps: 0.024905698351127572\n",
            "Validation loss per 100 evaluation steps: 0.02563598543425899\n",
            "Validation loss per 100 evaluation steps: 0.025739145745043338\n",
            "Validation loss per 100 evaluation steps: 0.025291956335214416\n",
            "Validation loss per 100 evaluation steps: 0.02527775029524367\n",
            "Validation loss per 100 evaluation steps: 0.025276161398618203\n",
            "Validation loss per 100 evaluation steps: 0.025060557009672343\n",
            "Validation loss per 100 evaluation steps: 0.02496543802546662\n",
            "Validation loss per 100 evaluation steps: 0.024961710883979936\n",
            "Validation loss per 100 evaluation steps: 0.025043443350480362\n",
            "Validation loss per 100 evaluation steps: 0.024905325038975665\n",
            "Validation loss per 100 evaluation steps: 0.024986479059652865\n",
            "Validation loss per 100 evaluation steps: 0.02539730410731856\n",
            "Validation loss per 100 evaluation steps: 0.025510335387021726\n",
            "Validation loss per 100 evaluation steps: 0.02538391406638995\n",
            "Validation loss per 100 evaluation steps: 0.025485657762118665\n",
            "Validation loss per 100 evaluation steps: 0.02554758766233273\n",
            "Validation loss per 100 evaluation steps: 0.025516154842411428\n",
            "Validation loss per 100 evaluation steps: 0.02537693578274773\n",
            "Validation loss per 100 evaluation steps: 0.02538854325526583\n",
            "Validation loss per 100 evaluation steps: 0.02528632154477556\n",
            "Validation loss per 100 evaluation steps: 0.025242759143338764\n",
            "Validation loss per 100 evaluation steps: 0.025189247577087997\n",
            "Validation loss per 100 evaluation steps: 0.025195017395579195\n",
            "Validation loss per 100 evaluation steps: 0.025355518121027\n",
            "Validation loss per 100 evaluation steps: 0.025137712188239274\n",
            "Validation loss per 100 evaluation steps: 0.02523075887168461\n",
            "Validation loss per 100 evaluation steps: 0.025245253396904952\n",
            "Validation loss per 100 evaluation steps: 0.02534370581787706\n",
            "Validation loss per 100 evaluation steps: 0.025263936896288593\n",
            "Validation loss per 100 evaluation steps: 0.025171572785307548\n",
            "Validation loss per 100 evaluation steps: 0.025028919308046762\n",
            "Validation loss per 100 evaluation steps: 0.02502011175213699\n",
            "Validation loss per 100 evaluation steps: 0.024981043926221178\n",
            "Validation loss per 100 evaluation steps: 0.024954250786122477\n",
            "Validation loss per 100 evaluation steps: 0.024909802889933396\n",
            "Validation loss per 100 evaluation steps: 0.024870871714352843\n",
            "Validation loss per 100 evaluation steps: 0.02483702306928858\n",
            "Validation loss per 100 evaluation steps: 0.02484846964837292\n",
            "Validation loss per 100 evaluation steps: 0.024814092904213252\n",
            "Validation Loss: 0.02471111085053838\n",
            "Validation Accuracy: 0.9617150367949096\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.0021447555627673864\n",
            "Training loss per 100 training steps: 0.018390007621359707\n",
            "Training loss per 100 training steps: 0.016042734336151988\n",
            "Training loss per 100 training steps: 0.016931899837365195\n",
            "Training loss per 100 training steps: 0.016856098474636837\n",
            "Training loss per 100 training steps: 0.01711728266220049\n",
            "Training loss per 100 training steps: 0.017356619406018882\n",
            "Training loss per 100 training steps: 0.01753970735092593\n",
            "Training loss per 100 training steps: 0.017281081034977323\n",
            "Training loss per 100 training steps: 0.017219847198495626\n",
            "Training loss per 100 training steps: 0.01695074143427118\n",
            "Training loss per 100 training steps: 0.01714244525720806\n",
            "Training loss per 100 training steps: 0.017017581950555975\n",
            "Training loss per 100 training steps: 0.017147208241358194\n",
            "Training loss per 100 training steps: 0.01706299816553985\n",
            "Training loss per 100 training steps: 0.016933194029114383\n",
            "Training loss per 100 training steps: 0.01701962299706245\n",
            "Training loss per 100 training steps: 0.016758919802882844\n",
            "Training loss per 100 training steps: 0.01674466982243907\n",
            "Training loss per 100 training steps: 0.01682154370752172\n",
            "Training loss per 100 training steps: 0.016959330835021134\n",
            "Training loss per 100 training steps: 0.016912167385469287\n",
            "Training loss per 100 training steps: 0.01697464110076593\n",
            "Training loss per 100 training steps: 0.017079520824415582\n",
            "Training loss per 100 training steps: 0.01715000097231593\n",
            "Training loss per 100 training steps: 0.017119724312231986\n",
            "Training loss per 100 training steps: 0.017136510157156306\n",
            "Training loss per 100 training steps: 0.017211228450891593\n",
            "Training loss per 100 training steps: 0.01715005663053285\n",
            "Training loss per 100 training steps: 0.017141687309905815\n",
            "Training loss per 100 training steps: 0.017207230937336913\n",
            "Training loss per 100 training steps: 0.017209720297628187\n",
            "Training loss per 100 training steps: 0.01717543391139268\n",
            "Training loss per 100 training steps: 0.017271017703666486\n",
            "Training loss per 100 training steps: 0.017209036662822814\n",
            "Training loss per 100 training steps: 0.01720222494278584\n",
            "Training loss per 100 training steps: 0.017206824581393765\n",
            "Training loss per 100 training steps: 0.017227545795418263\n",
            "Training loss per 100 training steps: 0.017189473507851473\n",
            "Training loss per 100 training steps: 0.017243764370924893\n",
            "Training loss per 100 training steps: 0.017175467108240357\n",
            "Training loss per 100 training steps: 0.01713609580822891\n",
            "Training loss per 100 training steps: 0.017164496641182636\n",
            "Training loss per 100 training steps: 0.017173021311824584\n",
            "Training loss per 100 training steps: 0.01720509898694524\n",
            "Training loss per 100 training steps: 0.017163147096449033\n",
            "Training loss per 100 training steps: 0.01716561619135571\n",
            "Training loss per 100 training steps: 0.017170244974106956\n",
            "Training loss per 100 training steps: 0.01721961437620688\n",
            "Training loss per 100 training steps: 0.01720335878490361\n",
            "Training loss per 100 training steps: 0.01721531667057295\n",
            "Training loss per 100 training steps: 0.017196122975313064\n",
            "Training loss per 100 training steps: 0.017162684986669303\n",
            "Training loss per 100 training steps: 0.017183544015602417\n",
            "Training loss per 100 training steps: 0.017192663489879398\n",
            "Training loss per 100 training steps: 0.017165504963523488\n",
            "Training loss per 100 training steps: 0.01716989663888558\n",
            "Training loss per 100 training steps: 0.017142206945074217\n",
            "Training loss per 100 training steps: 0.017157997661757806\n",
            "Training loss per 100 training steps: 0.017143521517774124\n",
            "Training loss per 100 training steps: 0.01709661407783791\n",
            "Training loss per 100 training steps: 0.017077441196379356\n",
            "Training loss per 100 training steps: 0.017050387027296103\n",
            "Training loss per 100 training steps: 0.01706004309144402\n",
            "Training loss per 100 training steps: 0.017063722655715152\n",
            "Training loss per 100 training steps: 0.017118057743320786\n",
            "Training loss per 100 training steps: 0.01713035683244109\n",
            "Training loss per 100 training steps: 0.017106942074175305\n",
            "Training loss per 100 training steps: 0.01711054208529914\n",
            "Training loss per 100 training steps: 0.01707547810355028\n",
            "Training loss per 100 training steps: 0.017068994373062894\n",
            "Training loss per 100 training steps: 0.017066677161624258\n",
            "Training loss per 100 training steps: 0.017085801195323443\n",
            "Training loss per 100 training steps: 0.017068264341220014\n",
            "Training loss per 100 training steps: 0.017085941135208137\n",
            "Training loss per 100 training steps: 0.017094061742598338\n",
            "Training loss per 100 training steps: 0.01710867835192309\n",
            "Training loss per 100 training steps: 0.01710916240323556\n",
            "Training loss per 100 training steps: 0.01710935928390678\n",
            "Training loss per 100 training steps: 0.017121690550648327\n",
            "Training loss per 100 training steps: 0.01715469593583425\n",
            "Training loss per 100 training steps: 0.01714672674487889\n",
            "Training loss per 100 training steps: 0.017146488567313423\n",
            "Training loss per 100 training steps: 0.017170133854098532\n",
            "Training loss per 100 training steps: 0.017158384640739665\n",
            "Training loss per 100 training steps: 0.017118412700162757\n",
            "Training loss per 100 training steps: 0.017150113533373194\n",
            "Training loss per 100 training steps: 0.017132634910398\n",
            "Training loss per 100 training steps: 0.01716108120130167\n",
            "Training loss per 100 training steps: 0.017170390833485392\n",
            "Training loss per 100 training steps: 0.017182024206948754\n",
            "Training loss per 100 training steps: 0.017159963078515476\n",
            "Training loss per 100 training steps: 0.01718975677862775\n",
            "Training loss per 100 training steps: 0.01716252050033583\n",
            "Training loss per 100 training steps: 0.017115507817154423\n",
            "Training loss per 100 training steps: 0.017124373283800922\n",
            "Training loss epoch: 0.01711671398995961\n",
            "Training accuracy epoch: 0.9725447265544551\n",
            "Validation loss per 100 evaluation steps: 0.01058136485517025\n",
            "Validation loss per 100 evaluation steps: 0.024165651609569024\n",
            "Validation loss per 100 evaluation steps: 0.024573006853548143\n",
            "Validation loss per 100 evaluation steps: 0.02372654315395659\n",
            "Validation loss per 100 evaluation steps: 0.02344027283475032\n",
            "Validation loss per 100 evaluation steps: 0.023843694116733816\n",
            "Validation loss per 100 evaluation steps: 0.024500677639170045\n",
            "Validation loss per 100 evaluation steps: 0.024342405478481943\n",
            "Validation loss per 100 evaluation steps: 0.02447594683743025\n",
            "Validation loss per 100 evaluation steps: 0.02444769258739029\n",
            "Validation loss per 100 evaluation steps: 0.02468318550876262\n",
            "Validation loss per 100 evaluation steps: 0.02430833477218676\n",
            "Validation loss per 100 evaluation steps: 0.023550845145514778\n",
            "Validation loss per 100 evaluation steps: 0.023502102935316356\n",
            "Validation loss per 100 evaluation steps: 0.023681276633383288\n",
            "Validation loss per 100 evaluation steps: 0.02334458955742348\n",
            "Validation loss per 100 evaluation steps: 0.02314528344915941\n",
            "Validation loss per 100 evaluation steps: 0.023098730333681892\n",
            "Validation loss per 100 evaluation steps: 0.023023442280022455\n",
            "Validation loss per 100 evaluation steps: 0.022640225974944708\n",
            "Validation loss per 100 evaluation steps: 0.022394911496676464\n",
            "Validation loss per 100 evaluation steps: 0.022285850981167477\n",
            "Validation loss per 100 evaluation steps: 0.022399703084036764\n",
            "Validation loss per 100 evaluation steps: 0.02266906737575147\n",
            "Validation loss per 100 evaluation steps: 0.022822241950129076\n",
            "Validation loss per 100 evaluation steps: 0.0229396666129759\n",
            "Validation loss per 100 evaluation steps: 0.02320834590471291\n",
            "Validation loss per 100 evaluation steps: 0.02340496415508817\n",
            "Validation loss per 100 evaluation steps: 0.023198947742591777\n",
            "Validation loss per 100 evaluation steps: 0.023014766671506058\n",
            "Validation loss per 100 evaluation steps: 0.022806068587462574\n",
            "Validation loss per 100 evaluation steps: 0.022975736070773762\n",
            "Validation loss per 100 evaluation steps: 0.02294369486767404\n",
            "Validation loss per 100 evaluation steps: 0.02294194069923377\n",
            "Validation loss per 100 evaluation steps: 0.02307446334530065\n",
            "Validation loss per 100 evaluation steps: 0.02310814132437828\n",
            "Validation loss per 100 evaluation steps: 0.023176737587979133\n",
            "Validation loss per 100 evaluation steps: 0.02316481554846403\n",
            "Validation loss per 100 evaluation steps: 0.023097898487492646\n",
            "Validation loss per 100 evaluation steps: 0.023146042424613614\n",
            "Validation loss per 100 evaluation steps: 0.023102613279028832\n",
            "Validation loss per 100 evaluation steps: 0.023118799926560428\n",
            "Validation loss per 100 evaluation steps: 0.023206645974178975\n",
            "Validation loss per 100 evaluation steps: 0.0231731632499616\n",
            "Validation loss per 100 evaluation steps: 0.023198448653898033\n",
            "Validation loss per 100 evaluation steps: 0.02333109254195539\n",
            "Validation loss per 100 evaluation steps: 0.023281885856041126\n",
            "Validation loss per 100 evaluation steps: 0.023467303975611677\n",
            "Validation Loss: 0.023469813949617595\n",
            "Validation Accuracy: 0.966020019121117\n",
            "Training epoch: 4\n",
            "Training loss per 100 training steps: 0.0071370238438248634\n",
            "Training loss per 100 training steps: 0.011467306899257802\n",
            "Training loss per 100 training steps: 0.011925876028052432\n",
            "Training loss per 100 training steps: 0.011813444540903793\n",
            "Training loss per 100 training steps: 0.0120236799301804\n",
            "Training loss per 100 training steps: 0.012221912526531773\n",
            "Training loss per 100 training steps: 0.012153747216280937\n",
            "Training loss per 100 training steps: 0.012049302041298567\n",
            "Training loss per 100 training steps: 0.012033184958271706\n",
            "Training loss per 100 training steps: 0.012217153749481761\n",
            "Training loss per 100 training steps: 0.012242237169519396\n",
            "Training loss per 100 training steps: 0.012288659928621446\n",
            "Training loss per 100 training steps: 0.012262558348164258\n",
            "Training loss per 100 training steps: 0.012207062046614712\n",
            "Training loss per 100 training steps: 0.012419725823149395\n",
            "Training loss per 100 training steps: 0.012396368984906012\n",
            "Training loss per 100 training steps: 0.012317605829707574\n",
            "Training loss per 100 training steps: 0.01240817344722837\n",
            "Training loss per 100 training steps: 0.012375530529804555\n",
            "Training loss per 100 training steps: 0.01234479785264905\n",
            "Training loss per 100 training steps: 0.012298897740515475\n",
            "Training loss per 100 training steps: 0.012296722933922784\n",
            "Training loss per 100 training steps: 0.012391424586330116\n",
            "Training loss per 100 training steps: 0.012393708722160036\n",
            "Training loss per 100 training steps: 0.01250572839610363\n",
            "Training loss per 100 training steps: 0.012604235794615157\n",
            "Training loss per 100 training steps: 0.01266149811832217\n",
            "Training loss per 100 training steps: 0.01273067554975286\n",
            "Training loss per 100 training steps: 0.012710367964675218\n",
            "Training loss per 100 training steps: 0.012692881598353786\n",
            "Training loss per 100 training steps: 0.012677885349567847\n",
            "Training loss per 100 training steps: 0.012602114222128584\n",
            "Training loss per 100 training steps: 0.012602711970240309\n",
            "Training loss per 100 training steps: 0.012574752024939389\n",
            "Training loss per 100 training steps: 0.012555701443420483\n",
            "Training loss per 100 training steps: 0.01252532930615784\n",
            "Training loss per 100 training steps: 0.012528372100222012\n",
            "Training loss per 100 training steps: 0.012538183172049524\n",
            "Training loss per 100 training steps: 0.012525664864354587\n",
            "Training loss per 100 training steps: 0.012506013402263818\n",
            "Training loss per 100 training steps: 0.012505409407830936\n",
            "Training loss per 100 training steps: 0.012495675684642623\n",
            "Training loss per 100 training steps: 0.012483733182277024\n",
            "Training loss per 100 training steps: 0.01250951022479397\n",
            "Training loss per 100 training steps: 0.012456983248735221\n",
            "Training loss per 100 training steps: 0.012532708767882808\n",
            "Training loss per 100 training steps: 0.012557871742701295\n",
            "Training loss per 100 training steps: 0.012598013178702369\n",
            "Training loss per 100 training steps: 0.012577740407254902\n",
            "Training loss per 100 training steps: 0.012596576874093584\n",
            "Training loss per 100 training steps: 0.012604121219695124\n",
            "Training loss per 100 training steps: 0.012589956468000709\n",
            "Training loss per 100 training steps: 0.01257396162172911\n",
            "Training loss per 100 training steps: 0.012517594886158388\n",
            "Training loss per 100 training steps: 0.01255342515966976\n",
            "Training loss per 100 training steps: 0.012582302941948518\n",
            "Training loss per 100 training steps: 0.012582563910642976\n",
            "Training loss per 100 training steps: 0.012583619838550109\n",
            "Training loss per 100 training steps: 0.012590133545916083\n",
            "Training loss per 100 training steps: 0.012591398845856855\n",
            "Training loss per 100 training steps: 0.012578027292400524\n",
            "Training loss per 100 training steps: 0.012569114757805906\n",
            "Training loss per 100 training steps: 0.012551715096290023\n",
            "Training loss per 100 training steps: 0.012564177759511887\n",
            "Training loss per 100 training steps: 0.012584718401676166\n",
            "Training loss per 100 training steps: 0.012556069893306482\n",
            "Training loss per 100 training steps: 0.012543821470849674\n",
            "Training loss per 100 training steps: 0.012593087496886618\n",
            "Training loss per 100 training steps: 0.012572714242606005\n",
            "Training loss per 100 training steps: 0.012587579491291865\n",
            "Training loss per 100 training steps: 0.012631816893604153\n",
            "Training loss per 100 training steps: 0.012661208381344772\n",
            "Training loss per 100 training steps: 0.012648034958240373\n",
            "Training loss per 100 training steps: 0.012631606239674634\n",
            "Training loss per 100 training steps: 0.012613842222066806\n",
            "Training loss per 100 training steps: 0.012611945691674085\n",
            "Training loss per 100 training steps: 0.01262838172473705\n",
            "Training loss per 100 training steps: 0.012631092443728509\n",
            "Training loss per 100 training steps: 0.012630098490330607\n",
            "Training loss per 100 training steps: 0.012642436745976827\n",
            "Training loss per 100 training steps: 0.012671672878374335\n",
            "Training loss per 100 training steps: 0.012683969395248428\n",
            "Training loss per 100 training steps: 0.01266535581741784\n",
            "Training loss per 100 training steps: 0.012677998122795792\n",
            "Training loss per 100 training steps: 0.012690155559700654\n",
            "Training loss per 100 training steps: 0.012721300479398555\n",
            "Training loss per 100 training steps: 0.012758819454398435\n",
            "Training loss per 100 training steps: 0.012756567525323307\n",
            "Training loss per 100 training steps: 0.012771144957175856\n",
            "Training loss per 100 training steps: 0.012757575464067509\n",
            "Training loss per 100 training steps: 0.01277544005866301\n",
            "Training loss per 100 training steps: 0.012782181966462516\n",
            "Training loss per 100 training steps: 0.012783949136051572\n",
            "Training loss per 100 training steps: 0.012792115444067631\n",
            "Training loss per 100 training steps: 0.012790364887948116\n",
            "Training loss per 100 training steps: 0.012813202496901566\n",
            "Training loss epoch: 0.012822100012060314\n",
            "Training accuracy epoch: 0.9790397227316743\n",
            "Validation loss per 100 evaluation steps: 0.0033164136111736298\n",
            "Validation loss per 100 evaluation steps: 0.030398611336263773\n",
            "Validation loss per 100 evaluation steps: 0.02661494102083461\n",
            "Validation loss per 100 evaluation steps: 0.027785059563498134\n",
            "Validation loss per 100 evaluation steps: 0.02809301107351174\n",
            "Validation loss per 100 evaluation steps: 0.028082581024973623\n",
            "Validation loss per 100 evaluation steps: 0.02676455104248753\n",
            "Validation loss per 100 evaluation steps: 0.026171769185876603\n",
            "Validation loss per 100 evaluation steps: 0.02622469855172705\n",
            "Validation loss per 100 evaluation steps: 0.026014201629575608\n",
            "Validation loss per 100 evaluation steps: 0.02657764436224426\n",
            "Validation loss per 100 evaluation steps: 0.026211797515586943\n",
            "Validation loss per 100 evaluation steps: 0.026128026990657666\n",
            "Validation loss per 100 evaluation steps: 0.025616503429212373\n",
            "Validation loss per 100 evaluation steps: 0.02536900247842361\n",
            "Validation loss per 100 evaluation steps: 0.025491310299203044\n",
            "Validation loss per 100 evaluation steps: 0.02546741192420028\n",
            "Validation loss per 100 evaluation steps: 0.024814934103890757\n",
            "Validation loss per 100 evaluation steps: 0.02454658502445425\n",
            "Validation loss per 100 evaluation steps: 0.024461384473376577\n",
            "Validation loss per 100 evaluation steps: 0.024549702462618783\n",
            "Validation loss per 100 evaluation steps: 0.024618885994862665\n",
            "Validation loss per 100 evaluation steps: 0.024538669092818096\n",
            "Validation loss per 100 evaluation steps: 0.024890080276181066\n",
            "Validation loss per 100 evaluation steps: 0.024958642726034826\n",
            "Validation loss per 100 evaluation steps: 0.024861916025083204\n",
            "Validation loss per 100 evaluation steps: 0.024817415225697914\n",
            "Validation loss per 100 evaluation steps: 0.024996488398735698\n",
            "Validation loss per 100 evaluation steps: 0.025048571779605722\n",
            "Validation loss per 100 evaluation steps: 0.02502971896758377\n",
            "Validation loss per 100 evaluation steps: 0.024945860999022653\n",
            "Validation loss per 100 evaluation steps: 0.02482885003023613\n",
            "Validation loss per 100 evaluation steps: 0.02486374124610873\n",
            "Validation loss per 100 evaluation steps: 0.024675759734326178\n",
            "Validation loss per 100 evaluation steps: 0.024955690503722556\n",
            "Validation loss per 100 evaluation steps: 0.024966865410348017\n",
            "Validation loss per 100 evaluation steps: 0.024942991218670805\n",
            "Validation loss per 100 evaluation steps: 0.024829411427555938\n",
            "Validation loss per 100 evaluation steps: 0.024735074878776894\n",
            "Validation loss per 100 evaluation steps: 0.024629727233616203\n",
            "Validation loss per 100 evaluation steps: 0.024708349732083985\n",
            "Validation loss per 100 evaluation steps: 0.024632629832857935\n",
            "Validation loss per 100 evaluation steps: 0.02471298130977692\n",
            "Validation loss per 100 evaluation steps: 0.02476838978249591\n",
            "Validation loss per 100 evaluation steps: 0.024656298085623835\n",
            "Validation loss per 100 evaluation steps: 0.02467235474779857\n",
            "Validation loss per 100 evaluation steps: 0.024715216964031964\n",
            "Validation loss per 100 evaluation steps: 0.024790515436071622\n",
            "Validation Loss: 0.02484043785683839\n",
            "Validation Accuracy: 0.9665633010961615\n",
            "Training epoch: 5\n",
            "Training loss per 100 training steps: 0.009961511939764023\n",
            "Training loss per 100 training steps: 0.009738922913588749\n",
            "Training loss per 100 training steps: 0.009048360748351459\n",
            "Training loss per 100 training steps: 0.009117189225441184\n",
            "Training loss per 100 training steps: 0.009048709389249222\n",
            "Training loss per 100 training steps: 0.009574517803333741\n",
            "Training loss per 100 training steps: 0.009303114299523478\n",
            "Training loss per 100 training steps: 0.009360465462007632\n",
            "Training loss per 100 training steps: 0.009211036182252596\n",
            "Training loss per 100 training steps: 0.009453617375643701\n",
            "Training loss per 100 training steps: 0.009419814952616526\n",
            "Training loss per 100 training steps: 0.009420989800370686\n",
            "Training loss per 100 training steps: 0.00948254446220964\n",
            "Training loss per 100 training steps: 0.009354580316025976\n",
            "Training loss per 100 training steps: 0.009288216451445374\n",
            "Training loss per 100 training steps: 0.009250773138332979\n",
            "Training loss per 100 training steps: 0.009279558053060575\n",
            "Training loss per 100 training steps: 0.009193465254633432\n",
            "Training loss per 100 training steps: 0.009168108146619126\n",
            "Training loss per 100 training steps: 0.009089847018367724\n",
            "Training loss per 100 training steps: 0.009102312507270928\n",
            "Training loss per 100 training steps: 0.009028707815182712\n",
            "Training loss per 100 training steps: 0.009037838245560393\n",
            "Training loss per 100 training steps: 0.009064621912999612\n",
            "Training loss per 100 training steps: 0.009079836070622321\n",
            "Training loss per 100 training steps: 0.009100931607030535\n",
            "Training loss per 100 training steps: 0.009139455663161038\n",
            "Training loss per 100 training steps: 0.009165702887002799\n",
            "Training loss per 100 training steps: 0.009135415288300709\n",
            "Training loss per 100 training steps: 0.009154035556845217\n",
            "Training loss per 100 training steps: 0.009170146038429066\n",
            "Training loss per 100 training steps: 0.009212267854056945\n",
            "Training loss per 100 training steps: 0.009215713590598648\n",
            "Training loss per 100 training steps: 0.009228830967895748\n",
            "Training loss per 100 training steps: 0.00920712928574975\n",
            "Training loss per 100 training steps: 0.00916245466158492\n",
            "Training loss per 100 training steps: 0.009186327004778956\n",
            "Training loss per 100 training steps: 0.009187129741505715\n",
            "Training loss per 100 training steps: 0.009208487042927758\n",
            "Training loss per 100 training steps: 0.009227677967252542\n",
            "Training loss per 100 training steps: 0.009245184104724996\n",
            "Training loss per 100 training steps: 0.009288774913077532\n",
            "Training loss per 100 training steps: 0.009304353042511577\n",
            "Training loss per 100 training steps: 0.009336632545451976\n",
            "Training loss per 100 training steps: 0.009362684814119612\n",
            "Training loss per 100 training steps: 0.009318263596491504\n",
            "Training loss per 100 training steps: 0.009320940527719035\n",
            "Training loss per 100 training steps: 0.009310511472862973\n",
            "Training loss per 100 training steps: 0.009305128148491574\n",
            "Training loss per 100 training steps: 0.009334711655040642\n",
            "Training loss per 100 training steps: 0.009360778324421564\n",
            "Training loss per 100 training steps: 0.009378710601809188\n",
            "Training loss per 100 training steps: 0.00936870271321082\n",
            "Training loss per 100 training steps: 0.009369089530959487\n",
            "Training loss per 100 training steps: 0.009374620277628136\n",
            "Training loss per 100 training steps: 0.009405854806184822\n",
            "Training loss per 100 training steps: 0.00944025145564855\n",
            "Training loss per 100 training steps: 0.009455861473930554\n",
            "Training loss per 100 training steps: 0.009483799713078916\n",
            "Training loss per 100 training steps: 0.009470504057554523\n",
            "Training loss per 100 training steps: 0.009454547944523807\n",
            "Training loss per 100 training steps: 0.009463619896746748\n",
            "Training loss per 100 training steps: 0.009470033275246738\n",
            "Training loss per 100 training steps: 0.00946125654890442\n",
            "Training loss per 100 training steps: 0.009473954631904917\n",
            "Training loss per 100 training steps: 0.00950537934593694\n",
            "Training loss per 100 training steps: 0.009514412969254966\n",
            "Training loss per 100 training steps: 0.009557150393084211\n",
            "Training loss per 100 training steps: 0.009561890384539482\n",
            "Training loss per 100 training steps: 0.009562852136505198\n",
            "Training loss per 100 training steps: 0.009565332922493867\n",
            "Training loss per 100 training steps: 0.009602674348178691\n",
            "Training loss per 100 training steps: 0.009594175494216262\n",
            "Training loss per 100 training steps: 0.009595831910735866\n",
            "Training loss per 100 training steps: 0.009620218847913694\n",
            "Training loss per 100 training steps: 0.009630078843979166\n",
            "Training loss per 100 training steps: 0.009648621114750735\n",
            "Training loss per 100 training steps: 0.009650697915919337\n",
            "Training loss per 100 training steps: 0.009654975574246192\n",
            "Training loss per 100 training steps: 0.009667905668733094\n",
            "Training loss per 100 training steps: 0.009659139271934942\n",
            "Training loss per 100 training steps: 0.009664344567672295\n",
            "Training loss per 100 training steps: 0.009654630130944182\n",
            "Training loss per 100 training steps: 0.009665049587725712\n",
            "Training loss per 100 training steps: 0.009666217461025054\n",
            "Training loss per 100 training steps: 0.009666180390778814\n",
            "Training loss per 100 training steps: 0.009678496952616708\n",
            "Training loss per 100 training steps: 0.009672189614817426\n",
            "Training loss per 100 training steps: 0.009679882520770301\n",
            "Training loss per 100 training steps: 0.009693222212531428\n",
            "Training loss per 100 training steps: 0.009682136630919415\n",
            "Training loss per 100 training steps: 0.009680411909262108\n",
            "Training loss per 100 training steps: 0.009680851289594648\n",
            "Training loss per 100 training steps: 0.00969246284128513\n",
            "Training loss per 100 training steps: 0.009710459467448218\n",
            "Training loss per 100 training steps: 0.009737101295392044\n",
            "Training loss epoch: 0.00973960443761075\n",
            "Training accuracy epoch: 0.9839517000554161\n",
            "Validation loss per 100 evaluation steps: 0.0011780629865825176\n",
            "Validation loss per 100 evaluation steps: 0.019960600233267848\n",
            "Validation loss per 100 evaluation steps: 0.020725255992131302\n",
            "Validation loss per 100 evaluation steps: 0.02510152119274067\n",
            "Validation loss per 100 evaluation steps: 0.027391930253988925\n",
            "Validation loss per 100 evaluation steps: 0.028423185561427592\n",
            "Validation loss per 100 evaluation steps: 0.02884945399619134\n",
            "Validation loss per 100 evaluation steps: 0.028911608583031073\n",
            "Validation loss per 100 evaluation steps: 0.02928081433053572\n",
            "Validation loss per 100 evaluation steps: 0.03002190174768553\n",
            "Validation loss per 100 evaluation steps: 0.030385476716017566\n",
            "Validation loss per 100 evaluation steps: 0.029767843095295583\n",
            "Validation loss per 100 evaluation steps: 0.028952268658981633\n",
            "Validation loss per 100 evaluation steps: 0.02849402798721774\n",
            "Validation loss per 100 evaluation steps: 0.028150653365811455\n",
            "Validation loss per 100 evaluation steps: 0.027855538509922784\n",
            "Validation loss per 100 evaluation steps: 0.027876529413526776\n",
            "Validation loss per 100 evaluation steps: 0.027824085646120347\n",
            "Validation loss per 100 evaluation steps: 0.027679368392247158\n",
            "Validation loss per 100 evaluation steps: 0.027477307129816067\n",
            "Validation loss per 100 evaluation steps: 0.027417044740703816\n",
            "Validation loss per 100 evaluation steps: 0.027597265801142697\n",
            "Validation loss per 100 evaluation steps: 0.027845748186506425\n",
            "Validation loss per 100 evaluation steps: 0.027666959159618507\n",
            "Validation loss per 100 evaluation steps: 0.027919259267198723\n",
            "Validation loss per 100 evaluation steps: 0.027986885924962648\n",
            "Validation loss per 100 evaluation steps: 0.028052477253591914\n",
            "Validation loss per 100 evaluation steps: 0.02791347188305533\n",
            "Validation loss per 100 evaluation steps: 0.02839992829094532\n",
            "Validation loss per 100 evaluation steps: 0.028261617645000823\n",
            "Validation loss per 100 evaluation steps: 0.028174848757409533\n",
            "Validation loss per 100 evaluation steps: 0.028192736738155012\n",
            "Validation loss per 100 evaluation steps: 0.028181201758134686\n",
            "Validation loss per 100 evaluation steps: 0.028323991607654826\n",
            "Validation loss per 100 evaluation steps: 0.028319285848292564\n",
            "Validation loss per 100 evaluation steps: 0.028278093505643927\n",
            "Validation loss per 100 evaluation steps: 0.02836013375053464\n",
            "Validation loss per 100 evaluation steps: 0.028184962548978145\n",
            "Validation loss per 100 evaluation steps: 0.028125954242870784\n",
            "Validation loss per 100 evaluation steps: 0.028197742788147104\n",
            "Validation loss per 100 evaluation steps: 0.028108647849849722\n",
            "Validation loss per 100 evaluation steps: 0.028075089901954446\n",
            "Validation loss per 100 evaluation steps: 0.028039383912214437\n",
            "Validation loss per 100 evaluation steps: 0.027887641273623854\n",
            "Validation loss per 100 evaluation steps: 0.027786500398336127\n",
            "Validation loss per 100 evaluation steps: 0.02781656490343658\n",
            "Validation loss per 100 evaluation steps: 0.027875856596201187\n",
            "Validation loss per 100 evaluation steps: 0.027855456718546976\n",
            "Validation Loss: 0.027842749859228043\n",
            "Validation Accuracy: 0.9661702786630613\n",
            "Training epoch: 6\n",
            "Training loss per 100 training steps: 0.00015038761193864048\n",
            "Training loss per 100 training steps: 0.005786523427514562\n",
            "Training loss per 100 training steps: 0.006233416985377521\n",
            "Training loss per 100 training steps: 0.006654126534820147\n",
            "Training loss per 100 training steps: 0.006631774495538972\n",
            "Training loss per 100 training steps: 0.006473432880920519\n",
            "Training loss per 100 training steps: 0.006675900856011951\n",
            "Training loss per 100 training steps: 0.006661630514745528\n",
            "Training loss per 100 training steps: 0.006728428074029523\n",
            "Training loss per 100 training steps: 0.006781678696111446\n",
            "Training loss per 100 training steps: 0.0068758304906195436\n",
            "Training loss per 100 training steps: 0.00683482902491732\n",
            "Training loss per 100 training steps: 0.006779397471993304\n",
            "Training loss per 100 training steps: 0.006849056859520057\n",
            "Training loss per 100 training steps: 0.006828472674734763\n",
            "Training loss per 100 training steps: 0.006849550768590472\n",
            "Training loss per 100 training steps: 0.0068460547980709685\n",
            "Training loss per 100 training steps: 0.006871937001839387\n",
            "Training loss per 100 training steps: 0.006839364192853667\n",
            "Training loss per 100 training steps: 0.006895473285726834\n",
            "Training loss per 100 training steps: 0.006970527151170954\n",
            "Training loss per 100 training steps: 0.007012355846341634\n",
            "Training loss per 100 training steps: 0.007053150919902157\n",
            "Training loss per 100 training steps: 0.007155670663382639\n",
            "Training loss per 100 training steps: 0.007128125832707113\n",
            "Training loss per 100 training steps: 0.00709009274003486\n",
            "Training loss per 100 training steps: 0.007087311856877403\n",
            "Training loss per 100 training steps: 0.007106201676843499\n",
            "Training loss per 100 training steps: 0.007115407197332963\n",
            "Training loss per 100 training steps: 0.007108424388440165\n",
            "Training loss per 100 training steps: 0.007139002073244456\n",
            "Training loss per 100 training steps: 0.007123873562437642\n",
            "Training loss per 100 training steps: 0.007113391068261742\n",
            "Training loss per 100 training steps: 0.007084954954344299\n",
            "Training loss per 100 training steps: 0.007079432309027003\n",
            "Training loss per 100 training steps: 0.007083423114134751\n",
            "Training loss per 100 training steps: 0.007075969084418501\n",
            "Training loss per 100 training steps: 0.0071338204710651495\n",
            "Training loss per 100 training steps: 0.007184574390278102\n",
            "Training loss per 100 training steps: 0.007201126114372893\n",
            "Training loss per 100 training steps: 0.007201400969118407\n",
            "Training loss per 100 training steps: 0.007189133190193599\n",
            "Training loss per 100 training steps: 0.007196975151366534\n",
            "Training loss per 100 training steps: 0.007192681672043744\n",
            "Training loss per 100 training steps: 0.0071991795026390476\n",
            "Training loss per 100 training steps: 0.007217828917937051\n",
            "Training loss per 100 training steps: 0.0072137030841404465\n",
            "Training loss per 100 training steps: 0.007193938332612191\n",
            "Training loss per 100 training steps: 0.007203684066133834\n",
            "Training loss per 100 training steps: 0.007221561330879551\n",
            "Training loss per 100 training steps: 0.007241091475902878\n",
            "Training loss per 100 training steps: 0.0072355929498496\n",
            "Training loss per 100 training steps: 0.0072331062348103435\n",
            "Training loss per 100 training steps: 0.007270801653701899\n",
            "Training loss per 100 training steps: 0.00726200826177625\n",
            "Training loss per 100 training steps: 0.007277597538406263\n",
            "Training loss per 100 training steps: 0.0072701417960927\n",
            "Training loss per 100 training steps: 0.007273397630218718\n",
            "Training loss per 100 training steps: 0.007293623963853503\n",
            "Training loss per 100 training steps: 0.007288882086195794\n",
            "Training loss per 100 training steps: 0.007283446683458238\n",
            "Training loss per 100 training steps: 0.007292523985215729\n",
            "Training loss per 100 training steps: 0.007296566589272054\n",
            "Training loss per 100 training steps: 0.007303602303365579\n",
            "Training loss per 100 training steps: 0.0072824412105933845\n",
            "Training loss per 100 training steps: 0.007291769543858204\n",
            "Training loss per 100 training steps: 0.007256237479190439\n",
            "Training loss per 100 training steps: 0.007275334922959286\n",
            "Training loss per 100 training steps: 0.007283462934114177\n",
            "Training loss per 100 training steps: 0.007283096481630301\n",
            "Training loss per 100 training steps: 0.007287402964600932\n",
            "Training loss per 100 training steps: 0.007318407504906485\n",
            "Training loss per 100 training steps: 0.007333466778110349\n",
            "Training loss per 100 training steps: 0.007318563333336608\n",
            "Training loss per 100 training steps: 0.0073390853313519515\n",
            "Training loss per 100 training steps: 0.007352153374287661\n",
            "Training loss per 100 training steps: 0.0073622775383596325\n",
            "Training loss per 100 training steps: 0.007370993588078835\n",
            "Training loss per 100 training steps: 0.0073985536804465025\n",
            "Training loss per 100 training steps: 0.007409926032081573\n",
            "Training loss per 100 training steps: 0.007424654852835952\n",
            "Training loss per 100 training steps: 0.007427135803650829\n",
            "Training loss per 100 training steps: 0.007449617666299786\n",
            "Training loss per 100 training steps: 0.00745751460686815\n",
            "Training loss per 100 training steps: 0.007476443759841463\n",
            "Training loss per 100 training steps: 0.007475687731941339\n",
            "Training loss per 100 training steps: 0.007467006669965157\n",
            "Training loss per 100 training steps: 0.007475981530998689\n",
            "Training loss per 100 training steps: 0.007463738662826714\n",
            "Training loss per 100 training steps: 0.007474911655191301\n",
            "Training loss per 100 training steps: 0.007455887808668091\n",
            "Training loss per 100 training steps: 0.007468216173001869\n",
            "Training loss per 100 training steps: 0.007472125177735238\n",
            "Training loss per 100 training steps: 0.007479582038296391\n",
            "Training loss per 100 training steps: 0.007496391697014527\n",
            "Training loss per 100 training steps: 0.007495567329570694\n",
            "Training loss epoch: 0.007495418797864272\n",
            "Training accuracy epoch: 0.9877923093563183\n",
            "Validation loss per 100 evaluation steps: 0.14575059711933136\n",
            "Validation loss per 100 evaluation steps: 0.030032387585239607\n",
            "Validation loss per 100 evaluation steps: 0.030355535356362966\n",
            "Validation loss per 100 evaluation steps: 0.028128459188984347\n",
            "Validation loss per 100 evaluation steps: 0.02933457694101767\n",
            "Validation loss per 100 evaluation steps: 0.029230514052881493\n",
            "Validation loss per 100 evaluation steps: 0.028541145617557426\n",
            "Validation loss per 100 evaluation steps: 0.028792691638281135\n",
            "Validation loss per 100 evaluation steps: 0.029342851360346114\n",
            "Validation loss per 100 evaluation steps: 0.02909201736098681\n",
            "Validation loss per 100 evaluation steps: 0.02896377755212018\n",
            "Validation loss per 100 evaluation steps: 0.029672948689151844\n",
            "Validation loss per 100 evaluation steps: 0.029783254697584118\n",
            "Validation loss per 100 evaluation steps: 0.029564561774626078\n",
            "Validation loss per 100 evaluation steps: 0.02999682691877227\n",
            "Validation loss per 100 evaluation steps: 0.030702785959163086\n",
            "Validation loss per 100 evaluation steps: 0.030466089761017594\n",
            "Validation loss per 100 evaluation steps: 0.030622281831741482\n",
            "Validation loss per 100 evaluation steps: 0.030539824107175782\n",
            "Validation loss per 100 evaluation steps: 0.030572430956676607\n",
            "Validation loss per 100 evaluation steps: 0.030849825400188425\n",
            "Validation loss per 100 evaluation steps: 0.031187299735236103\n",
            "Validation loss per 100 evaluation steps: 0.030884458971507295\n",
            "Validation loss per 100 evaluation steps: 0.030797411820151444\n",
            "Validation loss per 100 evaluation steps: 0.031055427576488\n",
            "Validation loss per 100 evaluation steps: 0.031120877999316565\n",
            "Validation loss per 100 evaluation steps: 0.03127161299536468\n",
            "Validation loss per 100 evaluation steps: 0.031207717410773817\n",
            "Validation loss per 100 evaluation steps: 0.031056341232434684\n",
            "Validation loss per 100 evaluation steps: 0.03109021754490352\n",
            "Validation loss per 100 evaluation steps: 0.03106091265636693\n",
            "Validation loss per 100 evaluation steps: 0.030733847350410112\n",
            "Validation loss per 100 evaluation steps: 0.030611459410613644\n",
            "Validation loss per 100 evaluation steps: 0.030349766813734885\n",
            "Validation loss per 100 evaluation steps: 0.03037196334986312\n",
            "Validation loss per 100 evaluation steps: 0.030464904785906143\n",
            "Validation loss per 100 evaluation steps: 0.03070535588450348\n",
            "Validation loss per 100 evaluation steps: 0.030737624928435025\n",
            "Validation loss per 100 evaluation steps: 0.031033477450487554\n",
            "Validation loss per 100 evaluation steps: 0.030921482234474523\n",
            "Validation loss per 100 evaluation steps: 0.030889031282672126\n",
            "Validation loss per 100 evaluation steps: 0.030851042982784085\n",
            "Validation loss per 100 evaluation steps: 0.030967477634024125\n",
            "Validation loss per 100 evaluation steps: 0.031080883369338737\n",
            "Validation loss per 100 evaluation steps: 0.03105044886386829\n",
            "Validation loss per 100 evaluation steps: 0.030903148500660615\n",
            "Validation loss per 100 evaluation steps: 0.03089758330401291\n",
            "Validation loss per 100 evaluation steps: 0.030819823386707714\n",
            "Validation Loss: 0.030817296409346766\n",
            "Validation Accuracy: 0.9659824546413904\n",
            "Training epoch: 7\n",
            "Training loss per 100 training steps: 0.0013876971788704395\n",
            "Training loss per 100 training steps: 0.005424394722800877\n",
            "Training loss per 100 training steps: 0.004863306167442091\n",
            "Training loss per 100 training steps: 0.005341868173953999\n",
            "Training loss per 100 training steps: 0.005662974832647934\n",
            "Training loss per 100 training steps: 0.005754531012679148\n",
            "Training loss per 100 training steps: 0.005796683210676031\n",
            "Training loss per 100 training steps: 0.005695408579753122\n",
            "Training loss per 100 training steps: 0.005569045070387233\n",
            "Training loss per 100 training steps: 0.005449125442166736\n",
            "Training loss per 100 training steps: 0.0055253912019756326\n",
            "Training loss per 100 training steps: 0.005562856628146538\n",
            "Training loss per 100 training steps: 0.005532193668350782\n",
            "Training loss per 100 training steps: 0.005497178722930357\n",
            "Training loss per 100 training steps: 0.0054693113016608405\n",
            "Training loss per 100 training steps: 0.005503145826765906\n",
            "Training loss per 100 training steps: 0.005496880215258007\n",
            "Training loss per 100 training steps: 0.005527642359812222\n",
            "Training loss per 100 training steps: 0.005506661426926797\n",
            "Training loss per 100 training steps: 0.005500373148924418\n",
            "Training loss per 100 training steps: 0.005528582503038156\n",
            "Training loss per 100 training steps: 0.00550137155438592\n",
            "Training loss per 100 training steps: 0.005494735787867401\n",
            "Training loss per 100 training steps: 0.005461476377541962\n",
            "Training loss per 100 training steps: 0.0054664395398395646\n",
            "Training loss per 100 training steps: 0.00550012269723309\n",
            "Training loss per 100 training steps: 0.005523907662750754\n",
            "Training loss per 100 training steps: 0.005525827452596445\n",
            "Training loss per 100 training steps: 0.0055490609845590365\n",
            "Training loss per 100 training steps: 0.005532928171296279\n",
            "Training loss per 100 training steps: 0.005564321455378718\n",
            "Training loss per 100 training steps: 0.005532301612760715\n",
            "Training loss per 100 training steps: 0.005558440042562696\n",
            "Training loss per 100 training steps: 0.005551833056375539\n",
            "Training loss per 100 training steps: 0.005570854378749337\n",
            "Training loss per 100 training steps: 0.005570949096640755\n",
            "Training loss per 100 training steps: 0.005578439716928791\n",
            "Training loss per 100 training steps: 0.005571533616446827\n",
            "Training loss per 100 training steps: 0.00557055735385425\n",
            "Training loss per 100 training steps: 0.005582412485981532\n",
            "Training loss per 100 training steps: 0.005581035949715954\n",
            "Training loss per 100 training steps: 0.005556687761618417\n",
            "Training loss per 100 training steps: 0.005574145269766801\n",
            "Training loss per 100 training steps: 0.005550810498456309\n",
            "Training loss per 100 training steps: 0.005561508889432366\n",
            "Training loss per 100 training steps: 0.005557998918855558\n",
            "Training loss per 100 training steps: 0.005581820110893663\n",
            "Training loss per 100 training steps: 0.005610313336742033\n",
            "Training loss per 100 training steps: 0.005623391843404426\n",
            "Training loss per 100 training steps: 0.0056355666197692176\n",
            "Training loss per 100 training steps: 0.005656044906401837\n",
            "Training loss per 100 training steps: 0.005661510093449325\n",
            "Training loss per 100 training steps: 0.005671805686295527\n",
            "Training loss per 100 training steps: 0.0057063474982892584\n",
            "Training loss per 100 training steps: 0.005712786414290734\n",
            "Training loss per 100 training steps: 0.005729184007070805\n",
            "Training loss per 100 training steps: 0.005732389542752571\n",
            "Training loss per 100 training steps: 0.005745776904104521\n",
            "Training loss per 100 training steps: 0.005735884646794096\n",
            "Training loss per 100 training steps: 0.005733008942608288\n",
            "Training loss per 100 training steps: 0.005741303559076521\n",
            "Training loss per 100 training steps: 0.005791864208045235\n",
            "Training loss per 100 training steps: 0.00578487620610027\n",
            "Training loss per 100 training steps: 0.00579338439305757\n",
            "Training loss per 100 training steps: 0.005784344209263118\n",
            "Training loss per 100 training steps: 0.005762786695563777\n",
            "Training loss per 100 training steps: 0.005747361856933307\n",
            "Training loss per 100 training steps: 0.005742049779870905\n",
            "Training loss per 100 training steps: 0.005751183111972418\n",
            "Training loss per 100 training steps: 0.005736930808351298\n",
            "Training loss per 100 training steps: 0.005753547379054098\n",
            "Training loss per 100 training steps: 0.005758774799385531\n",
            "Training loss per 100 training steps: 0.00574404214592595\n",
            "Training loss per 100 training steps: 0.005742298535892229\n",
            "Training loss per 100 training steps: 0.005747515181160881\n",
            "Training loss per 100 training steps: 0.005768174075537767\n",
            "Training loss per 100 training steps: 0.0057607533604984214\n",
            "Training loss per 100 training steps: 0.005755126757894967\n",
            "Training loss per 100 training steps: 0.0057875053966572245\n",
            "Training loss per 100 training steps: 0.005775185503011509\n",
            "Training loss per 100 training steps: 0.005775195628660027\n",
            "Training loss per 100 training steps: 0.005782619741581643\n",
            "Training loss per 100 training steps: 0.005785189639216496\n",
            "Training loss per 100 training steps: 0.005802096535812878\n",
            "Training loss per 100 training steps: 0.005803004237116193\n",
            "Training loss per 100 training steps: 0.005800261976020409\n",
            "Training loss per 100 training steps: 0.005803987342560545\n",
            "Training loss per 100 training steps: 0.0058025475692889215\n",
            "Training loss per 100 training steps: 0.005816581935050791\n",
            "Training loss per 100 training steps: 0.005810368362605848\n",
            "Training loss per 100 training steps: 0.0058284794248809595\n",
            "Training loss per 100 training steps: 0.00583317401809539\n",
            "Training loss per 100 training steps: 0.005824804200914393\n",
            "Training loss per 100 training steps: 0.005821394135508017\n",
            "Training loss per 100 training steps: 0.005822756552130238\n",
            "Training loss per 100 training steps: 0.0058187998184034495\n",
            "Training loss epoch: 0.0058208385111663445\n",
            "Training accuracy epoch: 0.9903924454262886\n",
            "Validation loss per 100 evaluation steps: 0.0427141934633255\n",
            "Validation loss per 100 evaluation steps: 0.030407587647624218\n",
            "Validation loss per 100 evaluation steps: 0.028521877612286233\n",
            "Validation loss per 100 evaluation steps: 0.02990378247481569\n",
            "Validation loss per 100 evaluation steps: 0.030317535475423395\n",
            "Validation loss per 100 evaluation steps: 0.03174009729726897\n",
            "Validation loss per 100 evaluation steps: 0.03283941753193062\n",
            "Validation loss per 100 evaluation steps: 0.03229738305965504\n",
            "Validation loss per 100 evaluation steps: 0.03207463573916199\n",
            "Validation loss per 100 evaluation steps: 0.031363880859057426\n",
            "Validation loss per 100 evaluation steps: 0.030833817611000524\n",
            "Validation loss per 100 evaluation steps: 0.030105864897293836\n",
            "Validation loss per 100 evaluation steps: 0.030209886211887534\n",
            "Validation loss per 100 evaluation steps: 0.03055363517384441\n",
            "Validation loss per 100 evaluation steps: 0.031057869223396845\n",
            "Validation loss per 100 evaluation steps: 0.032077437856662734\n",
            "Validation loss per 100 evaluation steps: 0.032179455240091824\n",
            "Validation loss per 100 evaluation steps: 0.03279916245502705\n",
            "Validation loss per 100 evaluation steps: 0.03402430572853252\n",
            "Validation loss per 100 evaluation steps: 0.034523061839687785\n",
            "Validation loss per 100 evaluation steps: 0.034366311562874945\n",
            "Validation loss per 100 evaluation steps: 0.03423194769555129\n",
            "Validation loss per 100 evaluation steps: 0.034327238684470136\n",
            "Validation loss per 100 evaluation steps: 0.033987282826785634\n",
            "Validation loss per 100 evaluation steps: 0.033780623730213545\n",
            "Validation loss per 100 evaluation steps: 0.03350965553991085\n",
            "Validation loss per 100 evaluation steps: 0.03336870290020475\n",
            "Validation loss per 100 evaluation steps: 0.033351658773973114\n",
            "Validation loss per 100 evaluation steps: 0.03364775297574872\n",
            "Validation loss per 100 evaluation steps: 0.033714884172928686\n",
            "Validation loss per 100 evaluation steps: 0.03362771986295111\n",
            "Validation loss per 100 evaluation steps: 0.03337171986769123\n",
            "Validation loss per 100 evaluation steps: 0.03333865694456024\n",
            "Validation loss per 100 evaluation steps: 0.033378603687689246\n",
            "Validation loss per 100 evaluation steps: 0.033398952630581076\n",
            "Validation loss per 100 evaluation steps: 0.033345200652383986\n",
            "Validation loss per 100 evaluation steps: 0.033203317883392924\n",
            "Validation loss per 100 evaluation steps: 0.03318127517391521\n",
            "Validation loss per 100 evaluation steps: 0.03343739930706379\n",
            "Validation loss per 100 evaluation steps: 0.0333786326069761\n",
            "Validation loss per 100 evaluation steps: 0.033467804563096136\n",
            "Validation loss per 100 evaluation steps: 0.03362878769113457\n",
            "Validation loss per 100 evaluation steps: 0.03365311100324352\n",
            "Validation loss per 100 evaluation steps: 0.03374594066126989\n",
            "Validation loss per 100 evaluation steps: 0.03359719333679458\n",
            "Validation loss per 100 evaluation steps: 0.0336580240519781\n",
            "Validation loss per 100 evaluation steps: 0.03379125081800369\n",
            "Validation loss per 100 evaluation steps: 0.03385612242042727\n",
            "Validation Loss: 0.033756910293431405\n",
            "Validation Accuracy: 0.9651245958975792\n",
            "Training epoch: 8\n",
            "Training loss per 100 training steps: 0.008960126899182796\n",
            "Training loss per 100 training steps: 0.003597295729116805\n",
            "Training loss per 100 training steps: 0.003722521482972415\n",
            "Training loss per 100 training steps: 0.0035502652527880565\n",
            "Training loss per 100 training steps: 0.003926460359835708\n",
            "Training loss per 100 training steps: 0.003941069699644441\n",
            "Training loss per 100 training steps: 0.003890280245518239\n",
            "Training loss per 100 training steps: 0.00392855367969246\n",
            "Training loss per 100 training steps: 0.003939760803602648\n",
            "Training loss per 100 training steps: 0.003926824833655252\n",
            "Training loss per 100 training steps: 0.004042966020865928\n",
            "Training loss per 100 training steps: 0.003947384944438853\n",
            "Training loss per 100 training steps: 0.003976546164739133\n",
            "Training loss per 100 training steps: 0.0039011104127495044\n",
            "Training loss per 100 training steps: 0.0039855716126375946\n",
            "Training loss per 100 training steps: 0.003967306222866688\n",
            "Training loss per 100 training steps: 0.003973708082655624\n",
            "Training loss per 100 training steps: 0.004003729077435521\n",
            "Training loss per 100 training steps: 0.004019851102029553\n",
            "Training loss per 100 training steps: 0.004052638584078729\n",
            "Training loss per 100 training steps: 0.004063577773810413\n",
            "Training loss per 100 training steps: 0.0040476244912640204\n",
            "Training loss per 100 training steps: 0.004015879019136396\n",
            "Training loss per 100 training steps: 0.004036547993592518\n",
            "Training loss per 100 training steps: 0.004060394595090254\n",
            "Training loss per 100 training steps: 0.004074942876003228\n",
            "Training loss per 100 training steps: 0.004072701954014553\n",
            "Training loss per 100 training steps: 0.004141942110612947\n",
            "Training loss per 100 training steps: 0.004164558207860054\n",
            "Training loss per 100 training steps: 0.004138022092839815\n",
            "Training loss per 100 training steps: 0.004188074810027313\n",
            "Training loss per 100 training steps: 0.004188388065725355\n",
            "Training loss per 100 training steps: 0.004224895411133878\n",
            "Training loss per 100 training steps: 0.004243290232570667\n",
            "Training loss per 100 training steps: 0.0042514858480799675\n",
            "Training loss per 100 training steps: 0.00427021918055466\n",
            "Training loss per 100 training steps: 0.004336586868693962\n",
            "Training loss per 100 training steps: 0.004345281716579666\n",
            "Training loss per 100 training steps: 0.004375223534199528\n",
            "Training loss per 100 training steps: 0.004386754289763142\n",
            "Training loss per 100 training steps: 0.004362795866882323\n",
            "Training loss per 100 training steps: 0.004344763354715736\n",
            "Training loss per 100 training steps: 0.004350315222146829\n",
            "Training loss per 100 training steps: 0.004355570339416923\n",
            "Training loss per 100 training steps: 0.004365527218830044\n",
            "Training loss per 100 training steps: 0.0043940422276621365\n",
            "Training loss per 100 training steps: 0.004376945617388497\n",
            "Training loss per 100 training steps: 0.004380773935363208\n",
            "Training loss per 100 training steps: 0.004388867816597871\n",
            "Training loss per 100 training steps: 0.004418466864597619\n",
            "Training loss per 100 training steps: 0.004419112399477004\n",
            "Training loss per 100 training steps: 0.0044205925661915954\n",
            "Training loss per 100 training steps: 0.004414852653502706\n",
            "Training loss per 100 training steps: 0.004403721082024694\n",
            "Training loss per 100 training steps: 0.004404663870632045\n",
            "Training loss per 100 training steps: 0.004431001173411566\n",
            "Training loss per 100 training steps: 0.004440993489884833\n",
            "Training loss per 100 training steps: 0.0044642040073786474\n",
            "Training loss per 100 training steps: 0.004462652850657703\n",
            "Training loss per 100 training steps: 0.0044647577415066675\n",
            "Training loss per 100 training steps: 0.004458861697913941\n",
            "Training loss per 100 training steps: 0.0044568969850036794\n",
            "Training loss per 100 training steps: 0.004465302220680964\n",
            "Training loss per 100 training steps: 0.004468462882971544\n",
            "Training loss per 100 training steps: 0.0044685121307784\n",
            "Training loss per 100 training steps: 0.004472164714129014\n",
            "Training loss per 100 training steps: 0.00447251493457688\n",
            "Training loss per 100 training steps: 0.004485973473324749\n",
            "Training loss per 100 training steps: 0.0044941409541993105\n",
            "Training loss per 100 training steps: 0.004491826149008444\n",
            "Training loss per 100 training steps: 0.004502437567587952\n",
            "Training loss per 100 training steps: 0.004488138620840776\n",
            "Training loss per 100 training steps: 0.004492304387074064\n",
            "Training loss per 100 training steps: 0.004481445054193647\n",
            "Training loss per 100 training steps: 0.004472709839365722\n",
            "Training loss per 100 training steps: 0.004472324488779474\n",
            "Training loss per 100 training steps: 0.0044707565515182245\n",
            "Training loss per 100 training steps: 0.004469626900314629\n",
            "Training loss per 100 training steps: 0.004478209079947691\n",
            "Training loss per 100 training steps: 0.004487261563431406\n",
            "Training loss per 100 training steps: 0.004486338265656204\n",
            "Training loss per 100 training steps: 0.004482269643039919\n",
            "Training loss per 100 training steps: 0.00449227055427566\n",
            "Training loss per 100 training steps: 0.004492365403888526\n",
            "Training loss per 100 training steps: 0.004500342404904471\n",
            "Training loss per 100 training steps: 0.004520800845058575\n",
            "Training loss per 100 training steps: 0.0045173625050592935\n",
            "Training loss per 100 training steps: 0.004525151611561491\n",
            "Training loss per 100 training steps: 0.004542909071437916\n",
            "Training loss per 100 training steps: 0.004549837351763635\n",
            "Training loss per 100 training steps: 0.004552418058139168\n",
            "Training loss per 100 training steps: 0.004562209218042823\n",
            "Training loss per 100 training steps: 0.004574520462763322\n",
            "Training loss per 100 training steps: 0.004571772177281461\n",
            "Training loss per 100 training steps: 0.004563529874160942\n",
            "Training loss per 100 training steps: 0.004559788110969968\n",
            "Training loss epoch: 0.004558326844121083\n",
            "Training accuracy epoch: 0.992390585231023\n",
            "Validation loss per 100 evaluation steps: 0.14374187588691711\n",
            "Validation loss per 100 evaluation steps: 0.050454349011488375\n",
            "Validation loss per 100 evaluation steps: 0.04438186451361345\n",
            "Validation loss per 100 evaluation steps: 0.04618622367463158\n",
            "Validation loss per 100 evaluation steps: 0.04226588167406456\n",
            "Validation loss per 100 evaluation steps: 0.04337827375915496\n",
            "Validation loss per 100 evaluation steps: 0.04210148618775242\n",
            "Validation loss per 100 evaluation steps: 0.04027564413004983\n",
            "Validation loss per 100 evaluation steps: 0.04003089278189157\n",
            "Validation loss per 100 evaluation steps: 0.039978734809908205\n",
            "Validation loss per 100 evaluation steps: 0.040430555878187294\n",
            "Validation loss per 100 evaluation steps: 0.04014585779444647\n",
            "Validation loss per 100 evaluation steps: 0.03991594504063937\n",
            "Validation loss per 100 evaluation steps: 0.039613584360990516\n",
            "Validation loss per 100 evaluation steps: 0.039975332248469025\n",
            "Validation loss per 100 evaluation steps: 0.03946469796352611\n",
            "Validation loss per 100 evaluation steps: 0.039201715012015526\n",
            "Validation loss per 100 evaluation steps: 0.03942368986091121\n",
            "Validation loss per 100 evaluation steps: 0.03942241368192601\n",
            "Validation loss per 100 evaluation steps: 0.03909775499705696\n",
            "Validation loss per 100 evaluation steps: 0.0390236816090579\n",
            "Validation loss per 100 evaluation steps: 0.03908013633913082\n",
            "Validation loss per 100 evaluation steps: 0.038955214079266186\n",
            "Validation loss per 100 evaluation steps: 0.038788983364384855\n",
            "Validation loss per 100 evaluation steps: 0.03907315436841677\n",
            "Validation loss per 100 evaluation steps: 0.03891215829515808\n",
            "Validation loss per 100 evaluation steps: 0.03883533220745793\n",
            "Validation loss per 100 evaluation steps: 0.0383776831607091\n",
            "Validation loss per 100 evaluation steps: 0.03807304080721473\n",
            "Validation loss per 100 evaluation steps: 0.03832323273223663\n",
            "Validation loss per 100 evaluation steps: 0.03797626283553362\n",
            "Validation loss per 100 evaluation steps: 0.03786948225477648\n",
            "Validation loss per 100 evaluation steps: 0.03796337143767146\n",
            "Validation loss per 100 evaluation steps: 0.037965152110640873\n",
            "Validation loss per 100 evaluation steps: 0.0378883545888163\n",
            "Validation loss per 100 evaluation steps: 0.0378881956615635\n",
            "Validation loss per 100 evaluation steps: 0.03768588694008854\n",
            "Validation loss per 100 evaluation steps: 0.03764545740768526\n",
            "Validation loss per 100 evaluation steps: 0.03766873282076361\n",
            "Validation loss per 100 evaluation steps: 0.037884798902635256\n",
            "Validation loss per 100 evaluation steps: 0.03794572525110732\n",
            "Validation loss per 100 evaluation steps: 0.037986983942027264\n",
            "Validation loss per 100 evaluation steps: 0.03791374760644818\n",
            "Validation loss per 100 evaluation steps: 0.03781423283293904\n",
            "Validation loss per 100 evaluation steps: 0.037717603608004045\n",
            "Validation loss per 100 evaluation steps: 0.03763481661283074\n",
            "Validation loss per 100 evaluation steps: 0.03763710599283\n",
            "Validation loss per 100 evaluation steps: 0.03733915027850871\n",
            "Validation Loss: 0.03732937927690563\n",
            "Validation Accuracy: 0.9648887493628124\n",
            "Training epoch: 9\n",
            "Training loss per 100 training steps: 0.005970652215182781\n",
            "Training loss per 100 training steps: 0.0031100956917671824\n",
            "Training loss per 100 training steps: 0.0032688757875703727\n",
            "Training loss per 100 training steps: 0.0030253810155498593\n",
            "Training loss per 100 training steps: 0.0029701777343791235\n",
            "Training loss per 100 training steps: 0.0030275735294770004\n",
            "Training loss per 100 training steps: 0.0031220354971742796\n",
            "Training loss per 100 training steps: 0.0033741946306900987\n",
            "Training loss per 100 training steps: 0.0033540760771117793\n",
            "Training loss per 100 training steps: 0.0033533136009940463\n",
            "Training loss per 100 training steps: 0.0034124305252857725\n",
            "Training loss per 100 training steps: 0.0033362435647181464\n",
            "Training loss per 100 training steps: 0.0033815393540161474\n",
            "Training loss per 100 training steps: 0.003340259982679491\n",
            "Training loss per 100 training steps: 0.003387700096957021\n",
            "Training loss per 100 training steps: 0.003410779312595589\n",
            "Training loss per 100 training steps: 0.0034117779960910794\n",
            "Training loss per 100 training steps: 0.003399805139377607\n",
            "Training loss per 100 training steps: 0.003350304134624111\n",
            "Training loss per 100 training steps: 0.003373026428898634\n",
            "Training loss per 100 training steps: 0.003392726700080582\n",
            "Training loss per 100 training steps: 0.0034211080101388017\n",
            "Training loss per 100 training steps: 0.0034930821681962414\n",
            "Training loss per 100 training steps: 0.0035132846485150804\n",
            "Training loss per 100 training steps: 0.003479702860161845\n",
            "Training loss per 100 training steps: 0.003458363226721175\n",
            "Training loss per 100 training steps: 0.003452530807041933\n",
            "Training loss per 100 training steps: 0.0035156786127777118\n",
            "Training loss per 100 training steps: 0.0035041472979629915\n",
            "Training loss per 100 training steps: 0.0034867362014720424\n",
            "Training loss per 100 training steps: 0.003478578318640007\n",
            "Training loss per 100 training steps: 0.0034724052208476145\n",
            "Training loss per 100 training steps: 0.0035019590272615717\n",
            "Training loss per 100 training steps: 0.0034831689696126866\n",
            "Training loss per 100 training steps: 0.003473880570158146\n",
            "Training loss per 100 training steps: 0.003453580693208058\n",
            "Training loss per 100 training steps: 0.0034231776001314436\n",
            "Training loss per 100 training steps: 0.0034521064138569725\n",
            "Training loss per 100 training steps: 0.0034831113149851253\n",
            "Training loss per 100 training steps: 0.003502941888656119\n",
            "Training loss per 100 training steps: 0.0035311506862556216\n",
            "Training loss per 100 training steps: 0.0035287383072235877\n",
            "Training loss per 100 training steps: 0.0035303686219072825\n",
            "Training loss per 100 training steps: 0.003543461985942713\n",
            "Training loss per 100 training steps: 0.003558706176679793\n",
            "Training loss per 100 training steps: 0.003557128422759117\n",
            "Training loss per 100 training steps: 0.0035474825802457193\n",
            "Training loss per 100 training steps: 0.003551519932930585\n",
            "Training loss per 100 training steps: 0.003542021209330246\n",
            "Training loss per 100 training steps: 0.003563382445610106\n",
            "Training loss per 100 training steps: 0.003547128575555136\n",
            "Training loss per 100 training steps: 0.003532766790939112\n",
            "Training loss per 100 training steps: 0.0035443123562213723\n",
            "Training loss per 100 training steps: 0.003546527675309535\n",
            "Training loss per 100 training steps: 0.0035386680204573854\n",
            "Training loss per 100 training steps: 0.0035614608832405473\n",
            "Training loss per 100 training steps: 0.003569226276636994\n",
            "Training loss per 100 training steps: 0.0035931242852626204\n",
            "Training loss per 100 training steps: 0.0036095109244443234\n",
            "Training loss per 100 training steps: 0.003628397688675944\n",
            "Training loss per 100 training steps: 0.0036210790416276017\n",
            "Training loss per 100 training steps: 0.003625668573099785\n",
            "Training loss per 100 training steps: 0.003618679180470982\n",
            "Training loss per 100 training steps: 0.0036158013417018965\n",
            "Training loss per 100 training steps: 0.003633549315854569\n",
            "Training loss per 100 training steps: 0.003653859419054871\n",
            "Training loss per 100 training steps: 0.003659545354251119\n",
            "Training loss per 100 training steps: 0.003652386121099924\n",
            "Training loss per 100 training steps: 0.0036467716150038005\n",
            "Training loss per 100 training steps: 0.0036385460230708155\n",
            "Training loss per 100 training steps: 0.0036317870079560824\n",
            "Training loss per 100 training steps: 0.003622100105298589\n",
            "Training loss per 100 training steps: 0.0036195894340954027\n",
            "Training loss per 100 training steps: 0.0036326076240238933\n",
            "Training loss per 100 training steps: 0.0036480962005762344\n",
            "Training loss per 100 training steps: 0.003652698552534909\n",
            "Training loss per 100 training steps: 0.0036449955302083593\n",
            "Training loss per 100 training steps: 0.0036605136126942875\n",
            "Training loss per 100 training steps: 0.0036537804917401643\n",
            "Training loss per 100 training steps: 0.0036553826457663466\n",
            "Training loss per 100 training steps: 0.0036510633487486987\n",
            "Training loss per 100 training steps: 0.0036556135400203195\n",
            "Training loss per 100 training steps: 0.0036509131112877073\n",
            "Training loss per 100 training steps: 0.00365648882546898\n",
            "Training loss per 100 training steps: 0.0036570374504377784\n",
            "Training loss per 100 training steps: 0.0036585813747354503\n",
            "Training loss per 100 training steps: 0.0036612183230284585\n",
            "Training loss per 100 training steps: 0.003669452730590019\n",
            "Training loss per 100 training steps: 0.0036631086487888325\n",
            "Training loss per 100 training steps: 0.003677123404768607\n",
            "Training loss per 100 training steps: 0.0036932753511926507\n",
            "Training loss per 100 training steps: 0.0036975376832612456\n",
            "Training loss per 100 training steps: 0.0036890337227326138\n",
            "Training loss per 100 training steps: 0.003703223276853355\n",
            "Training loss per 100 training steps: 0.0037050224900021904\n",
            "Training loss per 100 training steps: 0.0037007150515389756\n",
            "Training loss epoch: 0.003699557676310946\n",
            "Training accuracy epoch: 0.994024083289638\n",
            "Validation loss per 100 evaluation steps: 0.01198288518935442\n",
            "Validation loss per 100 evaluation steps: 0.04164660665931631\n",
            "Validation loss per 100 evaluation steps: 0.043765910214438125\n",
            "Validation loss per 100 evaluation steps: 0.038654555669969504\n",
            "Validation loss per 100 evaluation steps: 0.03945487313092286\n",
            "Validation loss per 100 evaluation steps: 0.03885089234797565\n",
            "Validation loss per 100 evaluation steps: 0.03938194063230056\n",
            "Validation loss per 100 evaluation steps: 0.03802965559819828\n",
            "Validation loss per 100 evaluation steps: 0.03844013842996013\n",
            "Validation loss per 100 evaluation steps: 0.03919552541019579\n",
            "Validation loss per 100 evaluation steps: 0.040293987323007194\n",
            "Validation loss per 100 evaluation steps: 0.03983794580255181\n",
            "Validation loss per 100 evaluation steps: 0.03926466314074563\n",
            "Validation loss per 100 evaluation steps: 0.0385869264898679\n",
            "Validation loss per 100 evaluation steps: 0.039479466408836025\n",
            "Validation loss per 100 evaluation steps: 0.039822954607576365\n",
            "Validation loss per 100 evaluation steps: 0.03948219839792717\n",
            "Validation loss per 100 evaluation steps: 0.03946774725990033\n",
            "Validation loss per 100 evaluation steps: 0.03985525057234517\n",
            "Validation loss per 100 evaluation steps: 0.039558331583443684\n",
            "Validation loss per 100 evaluation steps: 0.04005854521637846\n",
            "Validation loss per 100 evaluation steps: 0.039881115426375076\n",
            "Validation loss per 100 evaluation steps: 0.03959030022375749\n",
            "Validation loss per 100 evaluation steps: 0.03886618535107215\n",
            "Validation loss per 100 evaluation steps: 0.03878893929108286\n",
            "Validation loss per 100 evaluation steps: 0.03884248006976\n",
            "Validation loss per 100 evaluation steps: 0.039031873256250134\n",
            "Validation loss per 100 evaluation steps: 0.0389056006881807\n",
            "Validation loss per 100 evaluation steps: 0.03888621785136031\n",
            "Validation loss per 100 evaluation steps: 0.03879012545665163\n",
            "Validation loss per 100 evaluation steps: 0.038508511377276164\n",
            "Validation loss per 100 evaluation steps: 0.03845714062302344\n",
            "Validation loss per 100 evaluation steps: 0.03860020018873264\n",
            "Validation loss per 100 evaluation steps: 0.03866847913797284\n",
            "Validation loss per 100 evaluation steps: 0.0387280717679366\n",
            "Validation loss per 100 evaluation steps: 0.038751397250338854\n",
            "Validation loss per 100 evaluation steps: 0.0387683953795109\n",
            "Validation loss per 100 evaluation steps: 0.03866310623612458\n",
            "Validation loss per 100 evaluation steps: 0.03871234515549903\n",
            "Validation loss per 100 evaluation steps: 0.0386428521011071\n",
            "Validation loss per 100 evaluation steps: 0.03857864673710144\n",
            "Validation loss per 100 evaluation steps: 0.038560119428465565\n",
            "Validation loss per 100 evaluation steps: 0.03850958250130205\n",
            "Validation loss per 100 evaluation steps: 0.038612978410944694\n",
            "Validation loss per 100 evaluation steps: 0.03882594300183585\n",
            "Validation loss per 100 evaluation steps: 0.03860542617662002\n",
            "Validation loss per 100 evaluation steps: 0.03868557086995933\n",
            "Validation loss per 100 evaluation steps: 0.03861632078400639\n",
            "Validation Loss: 0.0386871123200506\n",
            "Validation Accuracy: 0.9649197984753808\n",
            "Training epoch: 10\n",
            "Training loss per 100 training steps: 4.63135693280492e-05\n",
            "Training loss per 100 training steps: 0.0024285856876430094\n",
            "Training loss per 100 training steps: 0.002294521637384207\n",
            "Training loss per 100 training steps: 0.0022588347125394007\n",
            "Training loss per 100 training steps: 0.00245587386534839\n",
            "Training loss per 100 training steps: 0.002638917317796483\n",
            "Training loss per 100 training steps: 0.0027970814885028843\n",
            "Training loss per 100 training steps: 0.002897466414707506\n",
            "Training loss per 100 training steps: 0.002797972235530142\n",
            "Training loss per 100 training steps: 0.002864337137919682\n",
            "Training loss per 100 training steps: 0.002869973320282899\n",
            "Training loss per 100 training steps: 0.0028141424730361232\n",
            "Training loss per 100 training steps: 0.002825608185589321\n",
            "Training loss per 100 training steps: 0.002821742472666241\n",
            "Training loss per 100 training steps: 0.002843275768903314\n",
            "Training loss per 100 training steps: 0.0028795165295671223\n",
            "Training loss per 100 training steps: 0.0029134399684453837\n",
            "Training loss per 100 training steps: 0.0029981691329303986\n",
            "Training loss per 100 training steps: 0.002992903333792374\n",
            "Training loss per 100 training steps: 0.003009640897089517\n",
            "Training loss per 100 training steps: 0.003020763619209409\n",
            "Training loss per 100 training steps: 0.0029778055324159803\n",
            "Training loss per 100 training steps: 0.002987351938906825\n",
            "Training loss per 100 training steps: 0.0029674580351727264\n",
            "Training loss per 100 training steps: 0.002953927689159233\n",
            "Training loss per 100 training steps: 0.002947035475683885\n",
            "Training loss per 100 training steps: 0.002914366866281055\n",
            "Training loss per 100 training steps: 0.002893947012414774\n",
            "Training loss per 100 training steps: 0.0028980705451647206\n",
            "Training loss per 100 training steps: 0.0028913373579482872\n",
            "Training loss per 100 training steps: 0.002862933172645211\n",
            "Training loss per 100 training steps: 0.002882837508775204\n",
            "Training loss per 100 training steps: 0.002883299460269505\n",
            "Training loss per 100 training steps: 0.002882860310264349\n",
            "Training loss per 100 training steps: 0.0028882456790554857\n",
            "Training loss per 100 training steps: 0.002888803806696465\n",
            "Training loss per 100 training steps: 0.002896777605728711\n",
            "Training loss per 100 training steps: 0.0029032682690808154\n",
            "Training loss per 100 training steps: 0.0028884060980868846\n",
            "Training loss per 100 training steps: 0.0028886546910061274\n",
            "Training loss per 100 training steps: 0.0028897773660415517\n",
            "Training loss per 100 training steps: 0.0028886536635780204\n",
            "Training loss per 100 training steps: 0.002911601694467572\n",
            "Training loss per 100 training steps: 0.0029111737704072705\n",
            "Training loss per 100 training steps: 0.002901660226607623\n",
            "Training loss per 100 training steps: 0.0029112572015680153\n",
            "Training loss per 100 training steps: 0.002902176306429228\n",
            "Training loss per 100 training steps: 0.00289241855808357\n",
            "Training loss per 100 training steps: 0.00288744950967801\n",
            "Training loss per 100 training steps: 0.002880967057816836\n",
            "Training loss per 100 training steps: 0.002876786229653293\n",
            "Training loss per 100 training steps: 0.0028581014093198207\n",
            "Training loss per 100 training steps: 0.0028629914026968317\n",
            "Training loss per 100 training steps: 0.002843087194750535\n",
            "Training loss per 100 training steps: 0.0028338871377132043\n",
            "Training loss per 100 training steps: 0.0028357100594892785\n",
            "Training loss per 100 training steps: 0.002845784688682759\n",
            "Training loss per 100 training steps: 0.0028507424691472435\n",
            "Training loss per 100 training steps: 0.0028524482565434376\n",
            "Training loss per 100 training steps: 0.0028531365689031035\n",
            "Training loss per 100 training steps: 0.00286788839463576\n",
            "Training loss per 100 training steps: 0.002880574440141668\n",
            "Training loss per 100 training steps: 0.002898088951302089\n",
            "Training loss per 100 training steps: 0.002915271821539618\n",
            "Training loss per 100 training steps: 0.0029164891208704902\n",
            "Training loss per 100 training steps: 0.002924494422196988\n",
            "Training loss per 100 training steps: 0.002913017051941969\n",
            "Training loss per 100 training steps: 0.0029136254787021044\n",
            "Training loss per 100 training steps: 0.002920361531851242\n",
            "Training loss per 100 training steps: 0.002912786799421835\n",
            "Training loss per 100 training steps: 0.002914344245651762\n",
            "Training loss per 100 training steps: 0.0029100631281365\n",
            "Training loss per 100 training steps: 0.0029016663731033024\n",
            "Training loss per 100 training steps: 0.002899301390299039\n",
            "Training loss per 100 training steps: 0.0029214490742801315\n",
            "Training loss per 100 training steps: 0.0029229513788229947\n",
            "Training loss per 100 training steps: 0.002940555093666495\n",
            "Training loss per 100 training steps: 0.002938383644540575\n",
            "Training loss per 100 training steps: 0.002935934436487006\n",
            "Training loss per 100 training steps: 0.0029301417525358967\n",
            "Training loss per 100 training steps: 0.0029528296778122643\n",
            "Training loss per 100 training steps: 0.002948684804508805\n",
            "Training loss per 100 training steps: 0.0029568171817993963\n",
            "Training loss per 100 training steps: 0.002957350712160758\n",
            "Training loss per 100 training steps: 0.00296101753781141\n",
            "Training loss per 100 training steps: 0.0029594672244905107\n",
            "Training loss per 100 training steps: 0.0029519728785355574\n",
            "Training loss per 100 training steps: 0.0029416641650906634\n",
            "Training loss per 100 training steps: 0.0029427732311904874\n",
            "Training loss per 100 training steps: 0.0029494327473357884\n",
            "Training loss per 100 training steps: 0.0029542965284993007\n",
            "Training loss per 100 training steps: 0.0029566434449302516\n",
            "Training loss per 100 training steps: 0.0029580862783026543\n",
            "Training loss per 100 training steps: 0.002967208514591682\n",
            "Training loss per 100 training steps: 0.002973276956023072\n",
            "Training loss per 100 training steps: 0.002972997423843613\n",
            "Training loss epoch: 0.0029728769755038745\n",
            "Training accuracy epoch: 0.9951372686874074\n",
            "Validation loss per 100 evaluation steps: 0.10544438660144806\n",
            "Validation loss per 100 evaluation steps: 0.04103631284917833\n",
            "Validation loss per 100 evaluation steps: 0.039059936944435755\n",
            "Validation loss per 100 evaluation steps: 0.03961434978644553\n",
            "Validation loss per 100 evaluation steps: 0.04157857906170988\n",
            "Validation loss per 100 evaluation steps: 0.04034646960028912\n",
            "Validation loss per 100 evaluation steps: 0.03949891015996005\n",
            "Validation loss per 100 evaluation steps: 0.03867473639175643\n",
            "Validation loss per 100 evaluation steps: 0.03888222530374276\n",
            "Validation loss per 100 evaluation steps: 0.03928628432049979\n",
            "Validation loss per 100 evaluation steps: 0.03985100972508045\n",
            "Validation loss per 100 evaluation steps: 0.03977645806608184\n",
            "Validation loss per 100 evaluation steps: 0.04091469683208398\n",
            "Validation loss per 100 evaluation steps: 0.041642153652108195\n",
            "Validation loss per 100 evaluation steps: 0.04082188437649722\n",
            "Validation loss per 100 evaluation steps: 0.039931822064393246\n",
            "Validation loss per 100 evaluation steps: 0.04021154101329605\n",
            "Validation loss per 100 evaluation steps: 0.04059534718099853\n",
            "Validation loss per 100 evaluation steps: 0.04049189639858364\n",
            "Validation loss per 100 evaluation steps: 0.03994969328752388\n",
            "Validation loss per 100 evaluation steps: 0.03988072555629161\n",
            "Validation loss per 100 evaluation steps: 0.04008753010539004\n",
            "Validation loss per 100 evaluation steps: 0.04013003045455397\n",
            "Validation loss per 100 evaluation steps: 0.04041955199841136\n",
            "Validation loss per 100 evaluation steps: 0.040851718082625874\n",
            "Validation loss per 100 evaluation steps: 0.04045932094141217\n",
            "Validation loss per 100 evaluation steps: 0.04053742970171306\n",
            "Validation loss per 100 evaluation steps: 0.04079750126074915\n",
            "Validation loss per 100 evaluation steps: 0.04122968200394391\n",
            "Validation loss per 100 evaluation steps: 0.041176033809655205\n",
            "Validation loss per 100 evaluation steps: 0.04124763727658489\n",
            "Validation loss per 100 evaluation steps: 0.04105150926308653\n",
            "Validation loss per 100 evaluation steps: 0.04096398565773001\n",
            "Validation loss per 100 evaluation steps: 0.04099191520708997\n",
            "Validation loss per 100 evaluation steps: 0.04089129077321023\n",
            "Validation loss per 100 evaluation steps: 0.04111275578853824\n",
            "Validation loss per 100 evaluation steps: 0.04073456410231747\n",
            "Validation loss per 100 evaluation steps: 0.040875135186535856\n",
            "Validation loss per 100 evaluation steps: 0.0408328090581774\n",
            "Validation loss per 100 evaluation steps: 0.0409207701210491\n",
            "Validation loss per 100 evaluation steps: 0.04070304100943472\n",
            "Validation loss per 100 evaluation steps: 0.040573293164159245\n",
            "Validation loss per 100 evaluation steps: 0.040438216041011756\n",
            "Validation loss per 100 evaluation steps: 0.04060451989095478\n",
            "Validation loss per 100 evaluation steps: 0.04073870453609501\n",
            "Validation loss per 100 evaluation steps: 0.04062118927234339\n",
            "Validation loss per 100 evaluation steps: 0.04047435946255556\n",
            "Validation loss per 100 evaluation steps: 0.040452917525283835\n",
            "Validation Loss: 0.04039719896937798\n",
            "Validation Accuracy: 0.9652426764465378\n"
          ]
        }
      ],
      "source": [
        "# Training -- and Evaluation\n",
        "\n",
        "Training_Results = []\n",
        "Validation_Results = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Training epoch: {epoch + 1}\")\n",
        "    epoch_result = train(epoch)\n",
        "    epoch_result[\"Epoch\"] = epoch + 1\n",
        "    Training_Results.append(epoch_result)\n",
        "\n",
        "    # Validation on 20% of data put aside in the beginning\n",
        "    labels, predictions, valid_result = valid(model, testing_loader)\n",
        "    valid_result[\"Epoch\"] = epoch + 1\n",
        "    # valid_result[\"labels\"] = labels\n",
        "    # valid_result[\"predictions\"] = predictions\n",
        "    Validation_Results.append(valid_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "0jDNXrjr-6BW",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'Validation loss epoch': 0.024871907058671323, 'Validation accuracy epoch': 0.9633732638163968, 'Epoch': 1}, {'Validation loss epoch': 0.02471111085053838, 'Validation accuracy epoch': 0.9617150367949096, 'Epoch': 2}, {'Validation loss epoch': 0.023469813949617595, 'Validation accuracy epoch': 0.966020019121117, 'Epoch': 3}, {'Validation loss epoch': 0.02484043785683839, 'Validation accuracy epoch': 0.9665633010961615, 'Epoch': 4}, {'Validation loss epoch': 0.027842749859228043, 'Validation accuracy epoch': 0.9661702786630613, 'Epoch': 5}, {'Validation loss epoch': 0.030817296409346766, 'Validation accuracy epoch': 0.9659824546413904, 'Epoch': 6}, {'Validation loss epoch': 0.033756910293431405, 'Validation accuracy epoch': 0.9651245958975792, 'Epoch': 7}, {'Validation loss epoch': 0.03732937927690563, 'Validation accuracy epoch': 0.9648887493628124, 'Epoch': 8}, {'Validation loss epoch': 0.0386871123200506, 'Validation accuracy epoch': 0.9649197984753808, 'Epoch': 9}, {'Validation loss epoch': 0.04039719896937798, 'Validation accuracy epoch': 0.9652426764465378, 'Epoch': 10}]\n",
            "[{'Training loss': 0.03897308698877425, 'Training accuracy': 0.9495700795696844, 'Epoch': 1}, {'Training loss': 0.02227375777897628, 'Training accuracy': 0.9657434621740814, 'Epoch': 2}, {'Training loss': 0.01711671398995961, 'Training accuracy': 0.9725447265544551, 'Epoch': 3}, {'Training loss': 0.012822100012060314, 'Training accuracy': 0.9790397227316743, 'Epoch': 4}, {'Training loss': 0.00973960443761075, 'Training accuracy': 0.9839517000554161, 'Epoch': 5}, {'Training loss': 0.007495418797864272, 'Training accuracy': 0.9877923093563183, 'Epoch': 6}, {'Training loss': 0.0058208385111663445, 'Training accuracy': 0.9903924454262886, 'Epoch': 7}, {'Training loss': 0.004558326844121083, 'Training accuracy': 0.992390585231023, 'Epoch': 8}, {'Training loss': 0.003699557676310946, 'Training accuracy': 0.994024083289638, 'Epoch': 9}, {'Training loss': 0.0029728769755038745, 'Training accuracy': 0.9951372686874074, 'Epoch': 10}]\n"
          ]
        }
      ],
      "source": [
        "# print(classification_report([labels], [predictions]))\n",
        "print(Validation_Results)\n",
        "print(Training_Results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNZQpDTt_ij8"
      },
      "source": [
        "## **Evaluation Phase II**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "JM3Huo0P_gsX"
      },
      "outputs": [],
      "source": [
        "# Imports for functionality\n",
        "import pandas as pd\n",
        "from matplotlib.pylab import plt\n",
        "from numpy import arange\n",
        "\n",
        "Training = pd.DataFrame.from_dict(Training_Results)\n",
        "Validation = pd.DataFrame.from_dict(Validation_Results)\n",
        "\n",
        "Training.to_csv('Training_final.csv', index=True)\n",
        "Validation.to_csv('Validation_final.csv', index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "GijqcsBtms9W"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAABCOElEQVR4nO3dd3wVVfr48c+TnpBGSAJpQIAAhg4REBABG6BSFFDWhvoVGxZcv7vu/nTXsn51XddVFHBBUbAhiiIqVgRRsBCqoUkIJY1AEBJKIO38/phJuIQEEshlUp736zWv3Jk5M/eZC7lPzjkz54gxBqWUUqq6PJwOQCmlVP2iiUMppVSNaOJQSilVI5o4lFJK1YgmDqWUUjWiiUMppVSNaOJQjhGRz0Xk5tou6yQR2SEil7jhvEtF5H/s19eLyFfVKXsG79NSRA6JiOeZxqoaPk0cqkbsL5WypVREClzWr6/JuYwxw4wxs2u7bF0kIn8RkWWVbA8XkUIR6Vzdcxlj3jbGXFZLcZ2Q6Iwxu4wxgcaYkto4f4X3MiLSrrbPq849TRyqRuwvlUBjTCCwC7jKZdvbZeVExMu5KOukN4F+IhJfYft1wK/GmBQHYlLqjGjiULVCRAaJSIaI/FlEdgOvi0hTEflURPaKyH77dazLMa7NLxNE5AcRec4uu11Ehp1h2XgRWSYiB0XkGxGZKiJvVRF3dWJ8UkSW2+f7SkTCXfbfKCI7RWSfiPy/qj4fY0wG8C1wY4VdNwGzTxdHhZgniMgPLuuXishmEckTkZcBcdnXVkS+tePLFZG3RSTU3vcm0BL4xK4x/klEWts1Ay+7TLSILBSR30UkVURudzn3YyIyT0Tm2J/NBhFJquozqIqIhNjn2Gt/lo+IiIe9r52IfGdfW66IvGdvFxH5j4jssfetr0mtTZ0dTRyqNrUAwoBWwESs/1+v2+stgQLg5VMc3wfYAoQDzwKviYicQdl3gF+AZsBjnPxl7ao6Mf4BuAWIBHyAhwBEJBGYbp8/2n6/Sr/sbbNdYxGRDkB34N1qxnESO4nNBx7B+iy2Af1diwBP2/GdB8RhfSYYY27kxFrjs5W8xbtAhn38GOD/RORil/0jgLlAKLCwOjFX4iUgBGgDXISVTG+x9z0JfAU0xfpsX7K3XwYMBNrb730tsO8M3ludCWOMLrqc0QLsAC6xXw8CCgG/U5TvDux3WV8K/I/9egKQ6rIvADBAi5qUxfrSLQYCXPa/BbxVzWuqLMZHXNbvBr6wX/8NmOuyr4n9GVxSxbkDgHygn73+FPDxGX5WP9ivbwJ+ciknWF/0/1PFeUcBayr7N7TXW9ufpRdWkikBglz2Pw28Yb9+DPjGZV8iUHCKz9YA7Sps8wSOAYku2+4Altqv5wAzgNgKxw0BfgP6Ah5O/y40tkVrHKo27TXGHC1bEZEAEfmv3fyQDywDQqXqO3Z2l70wxhyxXwbWsGw08LvLNoD0qgKuZoy7XV4fcYkp2vXcxpjDnOKvXjum94Gb7NrR9Vi1kDP5rMpUjMG4rotIpIjMFZFM+7xvYdVMqqPsszzosm0nEOOyXvGz8ZOa9W+FY9XidlbxHn/CSoa/2E1htwIYY77Fqt1MBXJEZIaIBNfgfdVZ0MShalPFoZb/CHQA+hhjgrGaFsClDd4NsoEwEQlw2RZ3ivJnE2O267nt92x2mmNmA+OAS4Eg4NOzjKNiDMKJ1/s01r9LV/u8N1Q456mGx87C+iyDXLa1BDJPE1NN5AJFWE10J72HMWa3MeZ2Y0w0Vk1kmth3ZhljphhjegGdsJqs/rcW41KnoIlDuVMQVlv9AREJA/7u7jc0xuwEkoHHRMRHRC4ArnJTjB8AV4rIABHxAZ7g9L9T3wMHsJpf5hpjCs8yjs+ATiJytf2X/n1YTXZlgoBD9nljOPnLNQerb+Ekxph0YAXwtIj4iUhX4Dbg7crKV5OPfS4/EfGzt80DnhKRIBFpBTyIVTNCRMa63CSwHyvRlYjI+SLSR0S8gcPAUaxmNXUOaOJQ7vQC4I/1V+VPwBfn6H2vBy7Aajb6B/AeVjt6ZV7gDGM0xmwA7sHqjM/G+mLLOM0xBqvdvpX986ziMMbkAmOBZ7CuNwFY7lLkcaAnkIeVZD6scIqngUdE5ICIPFTJW4zH6vfIAj4C/m6M+bo6sVVhA1aCLFtuAe7F+vJPA37A+jxn2eXPB34WkUNYne/3G2O2A8HATKzPfCfWtT93FnGpGhC7o0mpBsu+hXOzMcbtNR6lGgOtcagGx27GaCsiHiIyFBgJLHA4LKUaDH26VzVELbCaZJphNR3dZYxZ42xISjUc2lSllFKqRrSpSimlVI00iqaq8PBw07p1a6fDUEqpemXVqlW5xpiIitsbReJo3bo1ycnJToehlFL1iojsrGy7NlUppZSqEU0cSimlakQTh1JKqRrRxKGUUqpGNHEopZSqEU0cSimlakQTh1JKqRpxa+IQkaEissWe5P7hSvaLiEyx968XkZ4V9nuKyBoR+dRlW5iIfC0iW+2fTd15DUopVe8UHYW0pfDNY5CfVeund9sDgPaUl1OxZjrLAFaKyEJjzEaXYsOw5g9IAPoA0+2fZe4HNmGNvV/mYWCxMeYZOxk9DPzZXdehlFJ1Xmkp5PwK25ZYCWPXj1B8FDy8IK4vBEfX6tu588nx3kCqMSYNQETmYg1v7Zo4RgJz7MltfhKRUBGJMsZk27N+XQE8hTUjmOsxg+zXs4GlaOJQSjU2+3daSSJtCaR9BwW/W9sjzoOkW6HNIGjVH3wDa/2t3Zk4YoB0l/UMTqxNVFUmBms2tRewJqoPqnBMc2NMNoCdYCIre3MRmQhMBGjZsuWZXYFSStUVBfth+zIrWWxbAvu3W9uDoqD95dBmMLS5CIJanPI0tcGdiUMq2VZxDPdKy4jIlcAeY8wqERl0Jm9ujJmBNa8zSUlJOna8Uqp+KT4G6T8fb37KXgumFHwCofWF0OdOaDsYwtuDVPZV6j7uTBwZQJzLeizWvMXVKTMGGCEiwwE/IFhE3jLG3ADkuDRnRQF73HYFSil1rpSWQk6K3fy0FHaugOICEE+IPR8u+rPV/BTTCzy9HQ3VnYljJZAgIvFAJnAd8IcKZRYCk+z+jz5Ant0M9Rd7wa5xPGQnjbJjbgaesX9+7MZrUEop9zmQbvdRLLX6KY7kWtsjOkKvm63mp1b9wC/4lKc519yWOIwxxSIyCfgS8ARmGWM2iMid9v5XgEXAcCAVOALcUo1TPwPME5HbgF3AWHfEXyavoIgQf2ezu1KqgSg4ADu+P95P8fs2a3tgC2h3iVWjaHNRrd8FVdsaxdSxSUlJ5kzm4/j7xyl8u2UP3z00GA+Pc9uGqJRqAIqPQcbK4/0UWautfgrvJtB6gNVH0WaQVcM4x/0U1SEiq4wxSRW3N4qJnM5Uj5ZNmf3jTn7avo9+bcOdDkcpVdcVHoHsdVay2L4Mdi6HoiN2P0USDPxfq/kpphd4+Tgd7RnTxHEKQzu3IOhjL95PztDEoZQ6UWkJ7N0MmasgIxkyV8OejWBKrP3h7aHHjVaNonV/8AtxNNzapInjFPy8PRnRLZoPVmXw+MhOBPtpX4dSjZIxkJ9pJ4hVVpLIWgNFh639fqFWLaLDMOtnTE8IrPQRswZBE8dpXHt+HG//vItP1mVxfZ9WToejlDoXCg5YiSHTrklkroJDOdY+Tx9o0RV63GA1P8X0grA2dbKPwl00cZxGl5gQOrYIYl5yhiYOpRqi4kJrnKeyBJGRDPu2Ht8f3h7aDrFrEr2geed63T9RGzRxnIaIMDYpjic/3ciW3Qfp0KLiCChKqXrDGPg9zaXJaRXsXg8lhdb+JpFWLaLbdVaSiO4B/qGOhlwXaeKohlHdo3nm803MS07n0SsTnQ5HKVVdh/baCcKlb+LoAWufdxMrMfS583iTU3BMo2pyOlOaOKqhWaAvl5zXnI/WZPLnoR3x8dL5r5Sqk3I2wrbFdpPTKsjbZW0XT2ieCJ1G2U1OSRDRATw8HQ23vtLEUU3jkuL4PGU3327OYWjnKKfDUUq5ysuAxU/A+ves9dCWVi2izx1WoojqCj5NnI2xAdHEUU0XJoTTPNiXeckZmjiUqiuOHYQf/gM/TrXWBzwIfe9q0LfC1gWaOKrJy9ODMb1imb50Gzn5R2ke7Od0SEo1XiXFsHo2LH0aDu+FLuPg4r9BaNzpj1VnTRvra2BsrzhKDcxfneF0KEo1TsbAb1/BK/3hswehWQLc/i1cM1OTxjmkiaMGWoc3oXd8GO8nZ9AYBodUqk7Z/Su8OQreGQslRXDt23DLIqsPQ51TmjhqaFxSHNtzD7Nyx36nQ1GqccjPho/vgVcutAYQHPpPuPsnOO9KvXXWIZo4amh4lxY08fFkXnL66Qsrpc5c4WFY+gy81BPWz4ML7oH71kDfOxv9k9tO08RRQwE+XlzVLZrP1mdz6Fix0+Eo1fCUlsDqN2FKT6vzu/3lcM8vcPlT4N/U6egUmjjOyNikOAqKSvhsfcUp1JVSZ2XbEvjvQFg4yersvvUrGPsGhMU7HZlyoYnjDPRsGUq7yEDmJevdVUrVij2b4K0xVuf3sYMw5nW47Wto2cfpyFQl3Jo4RGSoiGwRkVQRebiS/SIiU+z960Wkp73dT0R+EZF1IrJBRB53OeYxEckUkbX2Mtyd11AZEWFcUiyrdu4ndc+hc/32SjUch/bAJw/A9H6Q/gtc+iRMWgmdr9aO7zrMbYlDRDyBqcAwIBEYLyIVRwgcBiTYy0Rgur39GDDEGNMN6A4MFZG+Lsf9xxjT3V4WuesaTmV0j1g8PYT3tZNcqZorKoBlz1n9GGvehPNvtzq++98HXr5OR6dOw501jt5AqjEmzRhTCMwFRlYoMxKYYyw/AaEiEmWvl/0p720vderBiYggX4Z0jGT+6kyKSkqdDkep+qG0FNa9By8lwbdPQvxAuPtnGP4sNGnmdHSqmtyZOGIA1z/HM+xt1SojIp4ishbYA3xtjPnZpdwku2lrlohUepuFiEwUkWQRSd67d+9ZXkrlxiXFkXvoGEu3uOf8SjUoO36AmYPho4nQJBwmfAbj34Hwdk5HpmrInYmjsgbKirWGKssYY0qMMd2BWKC3iHS2908H2mI1YWUD/67szY0xM4wxScaYpIiIiJpHXw2DOkQQHuirz3QodSq5qfDuH+CNK6xxpUbPgNuXQOsBTkemzpA7BznMAFwHj4kFKt6/etoyxpgDIrIUGAqkGGNyyvaJyEzg01qMuUa8PT24pmcMr/6wnT0HjxIZpAMfKlXu8D747p+Q/Bp4+cGQR62H+Lz9nY5MnSV31jhWAgkiEi8iPsB1wMIKZRYCN9l3V/UF8owx2SISISKhACLiD1wCbLbXXcc0Hw2kuPEaTmtsUhwlpYYFazKdDEOpuqP4GCyfAlN6wMqZ0ONGq+N74EOaNBoIt9U4jDHFIjIJ+BLwBGYZYzaIyJ32/leARcBwIBU4AtxiHx4FzLbvzPIA5hljymoWz4pId6wmrR3AHe66hupoFxlIr1ZNmZecwe0XtkH0FkLVWBkDGz6Ebx6DA7sg4TLr9trIjk5HpmqZW+fjsG+VXVRh2ysurw1wTyXHrQd6VHHOG2s5zLM2LimWP8//ldW7DtCrlQ6JoBoZY6yO78WPQ8ZKaN4ZblwAbQc7HZlyE31yvBZc0TUaf29PfaZDNS6lJbBhAcwcArOvhAPpMOJluGOZJo0GTmcArAWBvl5c0TWKT9Zl8berEgnw0Y9VNWBFBbD2bVjxMuzfDk3j4YrnofsftA+jkdBvuFoyLimOD1ZlsOjX3YzpFet0OErVviO/wy8z4ZcZcCQXonvCpY9DxyvBw9Pp6NQ5pImjlpzfuinx4U2Yl5yuiUM1LPt3wI9TYc1bUHQEEi63hgZp1V/Hk2qkNHHUEhFhbFIsz36xhe25h4kPb+J0SEqdnaw11m21GxeAeELXcdDvXog8z+nIlMO0c7wWXdMzFg+BD1ZpJ7mqp4yB1G9g9giYMch6fcEkeGA9jJqmSUMBWuOoVc2D/RjUIZIPVmUw+ZL2eHlqXlb1REkRpHwIK6ZATgoERcGlT0CvCeAX4nR0qo7RxFHLxiXFcudbe/h+ay6DO0Y6HY5Sp3bsIKyeAz9Og/wMiOgII6dBl7E6r7eqkiaOWjakY3PCmvgwLzldE4equw7mwM+vWONIHc2DVgPgyueh3aXgoTVldWqaOGqZj5cHo3vEMOfHHew7dIxmgTopjapDcrdazVHr5lrNU+ddBf3vh9gkpyNT9YgmDjcYlxTHaz9sZ8HaLG4bEO90OErBrp9h+YuwZZE1w16PG6xO72ZtnY5M1UOaONygQ4sgusWF8n5yOrf2b60DHypnlJbCb59bt9Sm/wT+TWHg/0LviRDonjlqVOOgicNNxiXF8v8+SuHXzDy6xoY6HY5qTIqOwvr3YMVLsG8rhLaEYc9atQwffb5InT1NHG5yVbdonvhkI++tTNfEoc6Ngv2QPAt+/i8cyoGobnDNa5A4Cjz1V13VHv3f5CbBft4M7xLFwrVZPHJFIv4+OpaPcpMD6fDTdFg9GwoPQduL4eoZEH+RDgmi3EIThxuNTYrlozWZfLlhN6N6xDgdjmpocjZaHd4pH1hPfHcZYw0J0qKL05GpBk4Thxv1jW9GXJg/85LTNXGo2mEM7PoRfngBtn4J3k2szu6+d0NonNPRqUbCrU/6iMhQEdkiIqki8nAl+0VEptj714tIT3u7n4j8IiLrRGSDiDzuckyYiHwtIlvtn3V2yj0PD2FcrzhWbNtH+u9HnA5H1WelpbD5M3jtMnh9GGSugsGPwOQUGPq0Jg11TrktcdjzhU8FhgGJwHgRSaxQbBiQYC8Tgen29mPAEGNMN6A7MFRE+tr7HgYWG2MSgMX2ep11Ta9YROD9VRlOh6Lqo+JCWP0mTOsDc/9gdXoPfw4e+BUu+l8ICHM6QtUIubPG0RtINcakGWMKgbnAyAplRgJzjOUnIFREouz1Q3YZb3sxLsfMtl/PBka58RrOWnSoPxcmRPBBcjolpeb0BygFcDTfev7ixa6wcJL10N41r8G9q6H37eAT4HSEqhFzZ+KIAVzHF8+wt1WrjIh4ishaYA/wtTHmZ7tMc2NMNoD9s9IBoURkoogki0jy3r17z/Zazsq4pFiy8o6yPDXX0ThUPXBoD3zzOPynM3z9KIQnwA0fwh3fW53felutqgPc+b+wsvsAK/7JXWUZY0wJ0F1EQoGPRKSzMSalum9ujJkBzABISkpy9E/9SxObExrgzbzkdAa21yd2VSX2bbMe2Fv7DpQUQuIIawypmF5OR6bUSdyZODIA1x67WCCrpmWMMQdEZCkwFEgBcuzmrGwRicKqkdRpvl6ejOoewzs/7+LAkUJCA3S4amXLXA3LX4CNC8HTB7qPh3736RhSqk5zZ1PVSiBBROJFxAe4DlhYocxC4Cb77qq+QJ6dECLsmgYi4g9cAmx2OeZm+/XNwMduvIZaMzYplsKSUj5eWzF3qkbHGEhdDLOvgpmDYdtSGPCA1eF91YuaNFSd57YahzGmWEQmAV8CnsAsY8wGEbnT3v8KsAgYDqQCR4Bb7MOjgNn2nVkewDxjzKf2vmeAeSJyG7ALGOuua6hNnaJD6BwTzLzkdG7u19rpcJQTSoqt+buXvwi710NgC3uWvVvAL9jp6JSqNrf2tBljFmElB9dtr7i8NsA9lRy3HuhRxTn3ARfXbqTnxrikOP728QZSMvPoHKPTcTYaRQWw5i348WXYvwOaJcCIl6HrOOtuKaXqGb1F4xwa0S2af3y2iQ9WZWjiaAyO/A4rX7Nm2juSC7Hnw2VPQYfhOsueqtc0cZxDoQE+XN6pBR+tyeThYR3x89aBDxukvAxrDu9Vb0DRYUi4DPo/AK366aCDqkHQxHGOjUuK5ZN1WXy9MYerukU7HY6qTXs2W/0Xv847Puhg//uheSenI1OqVmniOMf6tw0nJtQa+FATRwOx80crYfz2OXgHwPn/AxfcY02gpFQDpInjHPPwEMb0imXKt1vJPFBATKi/0yGpM1FaCr99YT2Dkf4z+IfBoL9YI9Xq+FGqgdMeOgeM6RWLMTBfBz6sf0qKYO27MP0CmDse8rOtaVknp8CghzVpqEZBaxwOiAsLoH+7Zry/Kp1Jg9vh4aEdpnVe4RFYPce6pTYvHSI7wdUzodNo8PR2OjqlzilNHA4ZlxTH/XPX8tP2ffRrG+50OKoqR36Hla/at9Tug5YXwBX/tu6U0jukVCOlicMhl3dqQZCfF/NWpmviqIvyMuGnaZD8unVLbfuh9i21FzgdmVKO08ThED9vT0Z2j+b95AweLygixF+bO+qEvb/Bihdh3XtgSvWWWqUqoYnDQeOS4njrp118si6LG/q2cjqcxi1zFfzwH9j0qTUMSNItcMEkaKr/LkpVpInDQV1iQujYIoj3k9M1cTjBGEhbYiWM7cvALwQGPgS974BAnTdFqapo4nCQiDAuKY4nPt3I5t35dGyhI6SeE6UlsGmhlTCy10FQFFz2D+g1AXyDnI5OqTpPn+Nw2KgeMXh7Cu8n6zMdbld8zBo/6uUkeH8CHDsEI16C+9dBv3s1aShVTVrjcFhYEx8uTWzOR2sy+fPQjvh4aS6vdUfzYdXr1sCDh3ZDVHcYNwc6XgkeOtCkUjWliaMOGJsUx6Jfd7N4Uw7DukQ5HU7DcWiv9fzFyplwNA/iL4LRr0CbQfoMhlJnQRNHHTAwIYIWwX7MS07XxFEb9u+AFS/Dmjet5qnzrrKmZo3p5XRkSjUIbm0XEZGhIrJFRFJF5OFK9ouITLH3rxeRnvb2OBFZIiKbRGSDiNzvcsxjIpIpImvtZbg7r+Fc8PQQrukVw3e/7WV33lGnw6m/cjbA/NthSk+rL6PLWJi0Eq59U5OGUrXIbYnDni98KjAMSATGi0hihWLDgAR7mQhMt7cXA380xpwH9AXuqXDsf4wx3e3lhKlp66uxveIoNTB/tXaS19jOH+HtcTC9H2xZBH3vggfWw8iXITzB6eiUanDc2VTVG0g1xqQBiMhcYCSw0aXMSGCOPff4TyISKiJRxphsIBvAGHNQRDYBMRWObVBahzehT3wY7yenc/egtoi2wZ9aaSls/cq6pTb9JwhoBoMfgfNv0xFqlXIzdyaOGCDdZT0D6FONMjHYSQNARFoDPYCfXcpNEpGbgGSsmsn+2gvbxZYvIHstePocX7x8Tlw/YZuvNVKql6+97n3yNg+vKjtmxyXF8cf317Fyx356x+uX30mMsaZl3f4d/DgV9myEkDgY9i/ocQP4BDgdoVKNgjsTR2XfjqYmZUQkEJgPPGCMybc3TweetMs9CfwbuPWkNxeZiNX8RcuWZzgTW+rX1siotUqqTEKjPX3o4HuUJvP8oXlTu4ydeAJbQFRXiOoGER0bx1DeR/Mha7U1HEjGKshMhkM51r6I82D0DOh8deP4LJSqQ9yZODKAOJf1WCCrumVExBsrabxtjPmwrIAxJqfstYjMBD6t7M2NMTOAGQBJSUkVE1b1XPFv66/Z0iLr7pySIig5BiWF1uvistf2Ulx44npJ4SmOc9lmH+dRUohXQS7Z+YeJKy7Cq6jAKlNcCFu/gV8OW3F5+kBkopVEypbmncC7Hs8mWFIMezacmCT2bqH874iwttZttDFJENsLonqAhz7zopQT3Jk4VgIJIhIPZALXAX+oUGYhVrPTXKxmrDxjTLZYDfyvAZuMMc+7HuDSBwIwGkhx4zVYX04evtZf/ufAkV37+cO0FTzTtQvX9XapKZWWwO9p1hAZZcumhbB6trVfPCGiA7SwayVRXaFFF2v8pbrGGGsypMxVkJFs/cxaC8UF1n7/MIhNgk5XW0kiuqf2WyhVh7gtcRhjikVkEvAl4AnMMsZsEJE77f2vAIuA4UAqcAS4xT68P3Aj8KuIrLW3/dW+g+pZEemO9afoDuAOd12DE3rEhdIuMpB5yeknJg4PT+sOofAEa6hvOP4FnL0OstdbP9OWwvq5x48La+OSTOylyTme/+NoHmStOZ4kMpLh8B5rn6evleR6TbCSRUwvaNpaH9BTqg4T64amhi0pKckkJyc7HUa1zVyWxlOLNvHNgwNpF3kG4ycdzIHd662O/bKEcmDn8f3BMRWSSVdrW218WZcUWc9TZK46niRyf6O8yalZO7u5yU4SzTtbfT1KqTpHRFYZY5Iqbtcnx+ugUT1i+OcXm3k/OYO/DD+v5icIag5Bl0LCpce3FeyH3b+6NHWth9++oPwLPaCZlURcE0rT+FP3IxgDB3ZZ/RGZq60kkb3ueJNTQDMrSXQZYyWJmJ7g37Tm16OUqlM0cdRBEUG+DOkYyfzVmTx0eQe8PWuhE9i/KcQPtJYyxw5ZtYPy2sk66zbX0iJrv0+Q3VdiJ5MWneFwrpUoyjqwD++1ynr6WmWSbrGSRGwShLbSJielGiBNHHXUuKQ4vtqYw5LNe7isUwv3vIlvILTsYy1lio/Bnk1WEtltN3OteuN4LaJMswRod8nxJBHZSZuclGokNHHUUYM6RBAR5Mu85Az3JY7KePlCdHdrKVNSDPtSISfFurspuoc2OSnViFUrcYhIE6DAGFMqIu2BjsDnxpgit0bXiHl5enB1zxhe/X47ew4eJTLIz7lgPL0gsqO1KKUaveo2ni8D/EQkBliMddvsG+4KSlnGJcVRUmr4aHWm06EopVS56iYOMcYcAa4GXjLGjMYa8Va5UduIQJJaNWVecjqN4bZppVT9UO3EISIXANcDn9nbtH/kHBiXFMe2vYdZveuA06EopRRQ/cTxAPAX4CP76e82wBK3RaXKDe8aRYCPJ9OXplJYXOp0OEopVb3EYYz5zhgzwhjzTxHxAHKNMfe5OTYFBPp6MWlIO77ZtIcbXv2Z3EPHnA5JKdXIVStxiMg7IhJs3121EdgiIv/r3tBUmbsHtePF67qzLuMAV730A79m5DkdklKqEatuU1WiPR/GKKyBCVtiDUKozpGR3WOYf1c/PEQY88oKPlqjU8wqpZxR3cThbc+PMQr42H5+Q2/zOcc6x4SwcFJ/erQMZfJ76/jHpxspLtF+D6XUuVXdxPFfrCHMmwDLRKQVkH/KI5RbNAv05c3b+jChX2te/WE7N7/+C/sPFzodllKqETnjYdVFxMsYU1zL8bhFfRtWvbrmJafzyEcpRAb7MuPGJBKjg50OSSnVgFQ1rHp1O8dDROR5EUm2l39j1T6Ug8YlxfHeHX0pKinlmukr+HR9xZl5lVKq9lW3qWoWcBAYZy/5wOvuCkpVX4+WTfnk3gEkRgcz6Z01PPvFZkpKtftJKeU+1U0cbY0xfzfGpNnL40Abdwamqi8yyI93b+/L+N4tmbZ0G7fNXklegY4/qZRyj+omjgIRGVC2IiL9gYJTlC8rN1REtohIqog8XMl+EZEp9v71ItLT3h4nIktEZJOIbBCR+12OCRORr0Vkq/1Tx/cGfLw8ePrqLjw1ujPLU3MZNXU5W3MOOh2WUqoBqm7iuBOYKiI7RGQH8DJwx6kOEBFPYCowDGtAxPEiUnFgxGFAgr1MBKbb24uBPxpjzgP6Ave4HPswsNgYk4A1Uu9JCakxu75PK965vS8HjxYzaupyvtyw2+mQlFINTHWHHFlnjOkGdAW6GmN6AENOc1hvINVu2ioE5gIjK5QZCcwxlp+AUBGJMsZkG2NW2+99ENgExLgcM9t+PRvr2RLl4vzWYXxyb3/aRQZyx5ur+M/Xv1Gq/R5KqVpSo8msjTH59hPkAA+epngMkO6ynsHxL/9qlxGR1kAP4Gd7U3NjTLYdTzYQWdmbi8jEsrvA9u7de5pQG56oEH/eu+MCrukZy4uLt3LHW6s4eFT7PZRSZ69GiaMCOYP9Ff/sPWUZEQkE5gMPuCSsajHGzDDGJBljkiIiImpyaIPh5+3Jc2O78verEvl28x5GT1tB2t5DToellKrnziZxnK7tIwOIc1mPBSo+aFBlGXuIk/nA28aYD13K5IhIlF0mCthT89AbDxHhlv7xvHlbb/YdOsbIqctZslk/MqXUmTtl4hCRgyKSX8lyEIg+zblXAgkiEi8iPsB1wMIKZRYCN9l3V/UF8owx2SIiwGvAJmPM85Ucc7P9+mbg49NfpurXNpyFkwYQ1zSAW2evZNrSVJ1VUCl1Rk6ZOIwxQcaY4EqWIGPMKWcAtIcjmQR8idW5Pc+eBOpOEbnTLrYISANSgZnA3fb2/lij7w4RkbX2Mtze9wxwqYhsBS6111U1xIUFMP+uflzVNZpnv9jCpHfWcKSwXowao5SqQ854rKr6pKGOVXWmjDHM/D6NZz7fTPvmQcy8KYm4sACnw1JK1TFnNVaValhEhIkD2/L6Lb3JOlDAVS//wPLUXKfDUkrVE5o4GrGL2kewcNIAIoN8ufG1n3n1+zTt91BKnZYmjkaudXgTPry7P5cltuAfn23iwXnrOFpU4nRYSqk6TBOHItDXi2nX9+SPl7ZnwdpMxryygswDpx2KTCnVSGniUAB4eAj3XpzAzBuT2Jl7hBEv/cDPafucDkspVQdp4lAnuCSxOR/d05+QAG+uf/Vn5vy4Q/s9lFIn0MShTtIuMpAF9/TnovYR/O3jDTw8/1eOFWu/h1LKoolDVSrYz5uZNyVx75B2vJecznUzfiIn/6jTYSml6gBNHKpKHh7CHy/rwPTre7Jl90Gu1H4PpRSaOFQ1DOsSxUd396eJjyfXzfyJJz7ZSEGhNl0p1Vhp4lDV0qFFEJ/ddyE39m3FrOXbGT7le1bt/N3psJRSDtDEoaqtia8XT4zszDu396GopJQxr/zI/y3apA8MKtXIaOJQNdavbThfPDCQP/RuyYxlaQyf8j2rd+13Oiyl1DmiiUOdkUBfL54a3YW3buvDsaJSxkxfwTOfb9bah1KNgCYOdVYGJITzxQMXcu35cbzy3TaueukH1qUfcDospZQbaeJQZy3Iz5unr+7K7Ft7c+hYMVdPX8G/vtysDw0q1UBp4lC15qL2EXw5eSDX9Ixh6pJtjHhpOb9m5DkdllKqlrk1cYjIUBHZIiKpIvJwJftFRKbY+9eLSE+XfbNEZI+IpFQ45jERyaxkSllVBwT7efPsmG68PuF8DhQUMmracp7/aguFxaVOh6aUqiVuSxwi4glMBYYBicB4EUmsUGwYkGAvE4HpLvveAIZWcfr/GGO628uiWg1c1YrBHSP56oGLGNk9minfpjLi5R/YkKW1D6UaAnfWOHoDqcaYNGNMITAXGFmhzEhgjrH8BISKSBSAMWYZoE+Y1WMhAd48P647r96UxL7DhYx8eTkvfPMbRSVa+1CqPnNn4ogB0l3WM+xtNS1TmUl209YsEWlaWQERmSgiySKSvHfv3prErWrZJYnN+XryQK7sGsUL32xl1NTlbMrOdzospdQZcmfikEq2VZzYoTplKpoOtAW6A9nAvysrZIyZYYxJMsYkRUREnOaUyt1CA3x44boe/PfGXuTkH2XEyz/w8rdbKdbah1L1jjsTRwYQ57IeC2SdQZkTGGNyjDElxphSYCZWk5iqJy7v1IKvJl/E0M5RPPfVb4yetoItuw86HZZSqgbcmThWAgkiEi8iPsB1wMIKZRYCN9l3V/UF8owx2ac6aVkfiG00kFJVWVU3hTXx4aXxPZh+fU+yDhRw1Us/MHVJqtY+lKon3JY4jDHFwCTgS2ATMM8Ys0FE7hSRO+1ii4A0IBWr9nB32fEi8i7wI9BBRDJE5DZ717Mi8quIrAcGA5PddQ3KvYZ1ieKryQO5JDGSf325hWumryB1j9Y+lKrrpDHMJ52UlGSSk5OdDkOdwqfrs3h0QQqHC0v446Xt+Z8L2+DpUVkXmFLqXBGRVcaYpIrb9clxVSdc2TWaryZfxOAOETz9+WbGvLKCbXsPOR2WUqoSmjhUnRER5MsrN/Tixeu6k7b3MMNf/J5Xv0+jpLTh14qVqk80cag6RUQY2T2GrycP5MKECP7x2Sau/e+PbM897HRoSimbJg5VJ0UG+zHzpl7859pu/JZzkGEvLmPWD9sp1dqHUo7TxKHqLBFhdI9Yvn7wIvq1DeeJTzdy3cyf2LlPax9KOUkTh6rzmgf78drNSfxrTFc2Zecz9IXv+deXm9l36JjToSnVKGniUPWCiDA2KY6vJg9kSMdIpi3dxoB/LuHJTzeSk3/U6fCUalT0OQ5VL6XuOci0Jdv4eF0WniKMSYrlrovaEhcW4HRoSjUYVT3HoYlD1Wu79h3hlWXb+CA5gxJjGNk9mrsHtaNdZKDToSlV72ni0MTRoO3OO8rM79N4++edHCsuZVjnFtwzuB2dokOcDk2peksThyaORmHfoWPMWr6dOSt2cvBYMUM6RnLP4Hb0alXptC1KqVPQxKGJo1HJKyhizoodzFq+nf1HirigTTMmDWlHv7bNENExsJSqDk0cmjgapcPHinn3l13MWJbGnoPH6NEylEmD2zGkY6QmEKVOQxOHJo5G7WhRCR+symD60m1kHijgvKhg7hnclmGdo3QUXqWqoIlDE4cCikpK+XhtFtOWppK29zBtIppw96B2jOwejbenPtaklCtNHJo4lIuSUsPnKdlMXbKNTdn5xDb1546L2jK2Vyx+3p5Oh6dUnaCJQxOHqoQxhm837+HlJams2XWAyCBfJg5swx/6tCTAx8vp8JRylCYOTRzqFIwx/LhtHy99m8qPaftoGuDNrf3jualfa0L8vZ0OTylHODIDoIgMFZEtIpIqIg9Xsl9EZIq9f72I9HTZN0tE9ohISoVjwkTkaxHZav/UG/TVWRMR+rUL592JfZl/Vz96tGzKv7/+jQHPfKsDKipVgdsSh4h4AlOBYUAiMF5EEisUGwYk2MtEYLrLvjeAoZWc+mFgsTEmAVhsrytVa3q1asqsCefz6b0DuLB9ePmAik98spHdeTqgolLurHH0BlKNMWnGmEJgLjCyQpmRwBxj+QkIFZEoAGPMMuD3Ss47Ephtv54NjHJH8Ep1jglh2vW9+HryQIZ1acHsH3cw8Nkl/PWjX0n//YjT4SnlGHcmjhgg3WU9w95W0zIVNTfGZAPYPyMrKyQiE0UkWUSS9+7dW6PAlXLVLjKI58d1Z+lDgxibFMsHyRkMem4pk99by4/b9umc6KrRcedtI5U9VVXxN6w6Zc6IMWYGMAOszvHaOKdq3OLCAnhqdBfuHZLAzO/TePeXXXy0JpPmwb5c1TWakd1j6BwTrE+kqwbPnYkjA4hzWY8Fss6gTEU5IhJljMm2m7X2nHWkStVAixA/Hr0ykT9e1p7Fm/bw8dosZv+4g1d/2E6b8CZc1S2akd2jaROhQ7urhsmdiWMlkCAi8UAmcB3whwplFgKTRGQu0AfIK2uGOoWFwM3AM/bPj2s1aqWqKcDHi6u6RXNVt2gOHCnk85TdLFybxZRvt/Li4q10iQlhZPdoruwaTYsQP6fDVarWuPU5DhEZDrwAeAKzjDFPicidAMaYV8Sq07+MdffUEeAWY0yyfey7wCAgHMgB/m6MeU1EmgHzgJbALmCsMaayTvRy+hyHOpd25x3l0/VZLFyXxfqMPESgT3wYI7vHMKxzC0IDfJwOUalq0QcANXEoB6TtPcTCdVksXJtFWu5hvD2Fi9pHMKJ7DJecF6lPp6s6TROHJg7lIGMMG7Ly+XhtJp+sy2Z3/lECfDy5LLE5I7pHc2FChA6yqOocTRyaOFQdUVpq+GXH73y8NotFv2aTV1BE0wBvhneJYmT3GJJaNcVDh3pXdYAmDk0cqg4qLC7l+617+XhtFl9vzKGgqIToED+u6hbNiO7RJEbp7b3KOZo4KiSOoqIiMjIyOHpUh5CoT/z8/IiNjcXbu+ENPHj4WDHfbMrh47VZLPttL8WlhnaRgYy0k0irZk2cDlE1Mpo4KiSO7du3ExQURLNmOgd1fWGMYd++fRw8eJD4+Hinw3Gr/YcLWZSSzcdrs/hlu3XTYLe4UEZ2i+bKrlFEBuvtvcr9NHFUSBybNm2iY8eOmjTqGWMMmzdv5rzzznM6lHMm60ABn67P4uO1WWzIysdD4IK2zRjZLYbLO7fQYd+V21SVOBr1vYCaNOqfxvhvFh3qz8SBbZk4sC2pe8pu783kT/PX88iCFAa2D+fChAj6tW1Gu8jARvkZqXOrUScOpeqbdpGBPHhpeyZfksD6jDwWrsviq427+WaTNfJOZJAv/do2o1+7cPq3Cycm1N/hiFVDpInDIfv27ePiiy8GYPfu3Xh6ehIREQHAL7/8go9P1U8XJycnM2fOHKZMmXLK9+jXrx8rVqw461iXLl3Kc889x6effnrW51K1Q0ToFhdKt7hQHr0ykfTfj7A8NZfl2/bxQ2ouC9ZaQ761bhZgJZG24VzQthlhTfSpdXX2NHE4pFmzZqxduxaAxx57jMDAQB566KHy/cXFxXh5Vf7Pk5SURFLSSc2OJ6mNpKHqh7iwAK7r3ZLrerfEGMOWnIMsT93HitRcFq7N4p2fdwGQGBVM/3ZWjaR36zCa+OpXgKo5/V8DPP7JBjZm5dfqOROjg/n7VZ1qdMyECRMICwtjzZo19OzZk2uvvZYHHniAgoIC/P39ef311+nQocMJNYDHHnuMXbt2kZaWxq5du3jggQe47777AAgMDOTQoUMsXbqUxx57jPDwcFJSUujVqxdvvfUWIsKiRYt48MEHCQ8Pp2fPnqSlpVW7ZvHuu+/yf//3fxhjuOKKK/jnP/9JSUkJt912G8nJyYgIt956K5MnT2bKlCm88soreHl5kZiYyNy5c2v8marqERE6tgimY4tgbhsQT3FJKesy8liRmsvybbnMXrGTmd9vx8tD6B4XatdImtGjZVN8vPTpdXV6mjjqmN9++41vvvkGT09P8vPzWbZsGV5eXnzzzTf89a9/Zf78+Scds3nzZpYsWcLBgwfp0KEDd91110nPOaxZs4YNGzYQHR1N//79Wb58OUlJSdxxxx0sW7aM+Ph4xo8fX+04s7Ky+POf/8yqVato2rQpl112GQsWLCAuLo7MzExSUqyp4g8cOADAM888w/bt2/H19S3fps4NL08PerVqSq9WTbn34gQKCktYtXM/y7flsiI1l5e/3cqUxVvx9/bk/Pgw+rdtRv924SRGBesT7KpSmjigxjUDdxo7diyenp4A5OXlcfPNN7N161ZEhKKiokqPueKKK/D19cXX15fIyEhycnKIjY09oUzv3r3Lt3Xv3p0dO3YQGBhImzZtyp+JGD9+PDNmzKhWnCtXrmTQoEHl/TLXX389y5Yt49FHHyUtLY17772XK664gssuuwyArl27cv311zNq1ChGjRpV489F1R5/H08GJIQzICEcgLwjRfy0fZ9dI9nH059vBiA0wJsL2jQrr5HEhzfRO7YUoImjzmnS5PjTwY8++iiDBw/mo48+YseOHQwaNKjSY3x9fctfe3p6UlxcXK0yZ/MMT1XHNm3alHXr1vHll18ydepU5s2bx6xZs/jss89YtmwZCxcu5Mknn2TDhg1V9uGocyskwJvLO7Xg8k4tAMjJP8qKbbnlfSSfp+wGICrEj35tw60+krbhOsdII6a/uXVYXl4eMTHWFOxvvPFGrZ+/Y8eOpKWlsWPHDlq3bs17771X7WP79OnD/fffT25uLk2bNuXdd9/l3nvvJTc3Fx8fH6655hratm3LhAkTKC0tJT09ncGDBzNgwADeeecdDh06RGhoaK1fkzp7zYP9GN0jltE9YjHGsGOfdcfWim25fLs5h/mrMwBoE9GE/nYiuaBNOCEB+iBiY6GJow7705/+xM0338zzzz/PkCFDav38/v7+TJs2jaFDhxIeHk7v3r2rLLt48eITmr/ef/99nn76aQYPHowxhuHDhzNy5EjWrVvHLbfcQmlpKQBPP/00JSUl3HDDDeTl5WGMYfLkyZo06gkRIT68CfHhTbihbytKSw0bs/PLayQfrMrgzZ92IgIdWwTTJSaYzjEhdI4J4bwWwfj7eDp9CcoNGvWQI41p2IqqHDp0iMDAQIwx3HPPPSQkJDB58mSnwzol/berOwqLS1mbfoDlqbms3rWflMw89h+x+uI8BBIig+gUE0wXO5kkRgXrLcD1iCNDjojIUOBFrKljXzXGPFNhv9j7h2NNHTvBGLP6VMeKyGPA7cBe+zR/NcYscud1NGQzZ85k9uzZFBYW0qNHD+644w6nQ1L1iI+XB73jw+gdHwZYfV9ZeUdJycwrX5b9lsuHqzMBEIE24U3KE0nnmBASo4MJ9tNmrvrEbTUOEfEEfgMuBTKAlcB4Y8xGlzLDgXuxEkcf4EVjTJ9THWsnjkPGmOeqG4vWOBoW/berf3LyrWTya2YeKZn5pGTmsTv/+JQG8eFN6BRtNXN1iQmhU3Swzs1eBzhR4+gNpBpj0uwA5gIjgY0uZUYCc4yVvX4SkVARiQJaV+NYpVQ90TzYj+bBflx8XvPybXsPHiMlK48NdkJZs+sAn67PLt8fF+ZP5+jjNZMuMSE6ZEod4c7EEQOku6xnYNUqTlcmphrHThKRm4Bk4I/GmP0V31xEJgITAVq2bHmGl6CUcpeIIF8Gd4hkcIfI8m37DxeSknW8VpKSlVd+OzBAdIjfCYmkU0wwkUF6W/C55s7EUdmTQhXbxaoqc6pjpwNP2utPAv8Gbj2psDEzgBlgNVVVL2SllJOaNvHhwoQILkyIKN+Wd6SIDdllfSZWQvlqY075/ubBvifUTNo3DyQ61B9vTx0+xV3cmTgygDiX9Vggq5plfKo61hhT/j9GRGYCOmSrUg1YSIA3/dqG069tePm2g0eL2JiVT0pWfnkn/JIteyi1/0T09BCiQ/1oGRZgL02Ov24WoJNfnSV3Jo6VQIKIxAOZwHXAHyqUWYjV7DQXqykqzxiTLSJ7qzpWRKKMMWUNoaOBFDdeg9sMGjSIv/zlL1x++eXl21544QV+++03pk2bVuUxzz33HElJSQwfPpx33nnnpOchKhtpt6IFCxbQvn17EhMTAfjb3/7GwIEDueSSS87qmnT4dXWuBPl506dNM/q0aVa+7UhhMZuy89m29zDpvx9h1+9H2LnvCF9tyGHf4cITjg/x9y5PImUJpVVYAHFhAUSF+OGltZVTclviMMYUi8gk4EusW2pnGWM2iMid9v5XgEVYd1SlYt2Oe8upjrVP/ayIdMdqqtoB1Mv7R8ePH8/cuXNPSBxz587lX//6V7WOX7TozO9AXrBgAVdeeWV54njiiSfO+FxK1RUBPl70ahVGr1ZhJ+07eLSI9N8L2PX7EXb9ftj+WcCGzDy+TNlNcenx1mwvDyG2qT9xZQnFTi5l60F667B7n+Own69YVGHbKy6vDXBPdY+1t99Yy2HC5w/D7l9r95wtusCwZ6rcPWbMGB555BGOHTuGr68vO3bsICsriwEDBnDXXXexcuVKCgoKGDNmDI8//vhJx7du3Zrk5GTCw8N56qmnmDNnDnFxcURERNCrVy/AekZjxowZFBYW0q5dO958803Wrl3LwoUL+e677/jHP/7B/PnzefLJJ7nyyisZM2YMixcv5qGHHqK4uJjzzz+f6dOn4+vrS+vWrbn55pv55JNPKCoq4v3336djx47V+ih0+HXltCA/bxKjvUmMDj5pX0mpITuvgF37jtgJ5Qg7fz9C+u9H+HR9NnkFJw4uGtbEhzi7huLa/NUyLIAWwX6NYkRhfYTTIc2aNaN379588cUXjBw5krlz53LttdciIjz11FOEhYVRUlLCxRdfzPr16+natWul51m1ahVz585lzZo1FBcX07Nnz/LEcfXVV3P77bcD8Mgjj/Daa69x7733MmLEiPJE4ero0aNMmDCBxYsX0759e2666SamT5/OAw88AEB4eDirV69m2rRpPPfcc7z66qunvU4dfl3VdZ4eQmzTAGKbBtCvkv15R4pI3281e5Ulll2/H2ZN+n4++zWbEpfaio+nB7Fh/rQMCyA61J/oED+iQvyJCvUjJtSfFiF++HrV/2FYNHHAKWsG7lTWXFWWOGbNmgXAvHnzmDFjBsXFxWRnZ7Nx48YqE8f333/P6NGjCQgIAGDEiBHl+1JSUnjkkUc4cOAAhw4dOqFZrDJbtmwhPj6e9u3bA3DzzTczderU8sRx9dVXA9CrVy8+/PDDal2jDr+u6ruQAG9CAqw7tioqKikl60DB8YSy73jfytr0Axw4cvJUCOGBPlYyCfEjOtT6GVWWZEL9aR7kW+f7WDRxOGjUqFE8+OCDrF69moKCAnr27Mn27dt57rnnWLlyJU2bNmXChAkcPXr0lOepao6ECRMmsGDBArp168Ybb7zB0qVLT3me040iUDY0e1VDt9fknDr8umoIvD09aNWsCa2aNal0f0FhCVl5BWQfOFr+MzuvgKy8o2zPPcyKbfs4dOzE3yUPgcggP6JC/YgOOTmxRIf4ER7o62iTmP5GOigwMJBBgwZx6623ls++l5+fT5MmTQgJCSEnJ4fPP/+8ynk4AAYOHMiECRN4+OGHKS4u5pNPPikfb+rgwYNERUVRVFTE22+/XT5Ee1BQEAcPHjzpXB07dmTHjh2kpqaW94lcdNFFZ3WNOvy6asz8fTxpGxFI24jAKsvkHy06ObHYPzdm5/PNphyOFZeecIy3p9DCbgZzTShlzWLRIf6EBni7beItTRwOGz9+PFdffXV5J3C3bt3o0aMHnTp1ok2bNvTv3/+Ux5fNTd69e3datWrFhRdeWL7vySefpE+fPrRq1YouXbqUJ4vrrruO22+/nSlTpvDBBx+Ul/fz8+P1119n7Nix5Z3jd955Z42uR4dfV6pmgv28CW7hTYcWQZXuN8aw/0gRWQcKyM47StaBghOSzMod+8nJzz7hzjAAP28PokP8eWp0Fy5o26zSc58pHVZd1Tv6b6fUiUpKDbmHjp2QXLLzrMRy38UJdGxx8t1k1eHIsOpKKaXcz9NDygeS7HEO3q9ud90rpZSqcxp14mgMzXQNjf6bKeW8Rps4/Pz82Ldvn34R1SPGGPbt24efnw6jrZSTGm0fR2xsLBkZGezdu/f0hVWd4efnd8JdW0qpc6/RJg5vb2/i4+OdDkMppeqdRttUpZRS6sxo4lBKKVUjmjiUUkrVSKN4ctyeUXDnGR4eDuTWYjj1nX4ex+lncSL9PE7UED6PVsaYiIobG0XiOBsiklzZI/eNlX4ex+lncSL9PE7UkD8PbapSSilVI5o4lFJK1YgmjtOb4XQAdYx+HsfpZ3Ei/TxO1GA/D+3jUEopVSNa41BKKVUjmjiUUkrViCaOUxCRoSKyRURSReRhp+NxiojEicgSEdkkIhtE5H6nY6oLRMRTRNaIyKdOx+I0EQkVkQ9EZLP9/+QCp2NyiohMtn9PUkTkXRFpcMM5a+Kogoh4AlOBYUAiMF5EEp2NyjHFwB+NMecBfYF7GvFn4ep+YJPTQdQRLwJfGGM6At1opJ+LiMQA9wFJxpjOgCdwnbNR1T5NHFXrDaQaY9KMMYXAXGCkwzE5whiTbYxZbb8+iPWlEONsVM4SkVjgCuBVp2NxmogEAwOB1wCMMYXGmAOOBuUsL8BfRLyAACDL4XhqnSaOqsUA6S7rGTTyL0sAEWkN9AB+djgUp70A/AkodTiOuqANsBd43W66e1VEmjgdlBOMMZnAc8AuIBvIM8Z85WxUtU8TR9Wkkm2N+t5lEQkE5gMPGGPynY7HKSJyJbDHGLPK6VjqCC+gJzDdGNMDOAw0yj5BEWmK1TIRD0QDTUTkBmejqn2aOKqWAcS5rMfSAKuc1SUi3lhJ421jzIdOx+Ow/sAIEdmB1YQ5RETecjYkR2UAGcaYslroB1iJpDG6BNhujNlrjCkCPgT6ORxTrdPEUbWVQIKIxIuID1YH10KHY3KEiAhW+/UmY8zzTsfjNGPMX4wxscaY1lj/L741xjS4vyqryxizG0gXkQ72pouBjQ6G5KRdQF8RCbB/by6mAd4o0Ginjj0dY0yxiEwCvsS6M2KWMWaDw2E5pT9wI/CriKy1t/3VGLPIuZBUHXMv8Lb9R1YacIvD8TjCGPOziHwArMa6G3ENDXDoER1yRCmlVI1oU5VSSqka0cShlFKqRjRxKKWUqhFNHEoppWpEE4dSSqka0cSh1FkQkRIRWeuy1NoT0yLSWkRSaut8StUWfY5DqbNTYIzp7nQQSp1LWuNQyg1EZIeI/FNEfrGXdvb2ViKyWETW2z9b2tubi8hHIrLOXsqGqfAUkZn2/A5fiYi/Xf4+Edlon2euQ5epGilNHEqdHf8KTVXXuuzLN8b0Bl7GGk0X+/UcY0xX4G1gir19CvCdMaYb1jhPZaMUJABTjTGdgAPANfb2h4Ee9nnudM+lKVU5fXJcqbMgIoeMMYGVbN8BDDHGpNkDRO42xjQTkVwgyhhTZG/PNsaEi8heINYYc8zlHK2Br40xCfb6nwFvY8w/ROQL4BCwAFhgjDnk5ktVqpzWOJRyH1PF66rKVOaYy+sSjvdLXoE1Q2UvYJU9aZBS54QmDqXc51qXnz/ar1dwfCrR64Ef7NeLgbugfC7z4KpOKiIeQJwxZgnWZFKhwEm1HqXcRf9KUers+LuMGAzWvNtlt+T6isjPWH+gjbe33QfMEpH/xZo1r2wU2fuBGSJyG1bN4i6sGeQq4wm8JSIhWBOO/aeRT9WqzjHt41DKDew+jiRjTK7TsShV27SpSimlVI1ojUMppVSNaI1DKaVUjWjiUEopVSOaOJRSStWIJg6llFI1oolDKaVUjfx/b0mrhOGD/5QAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# resort into 4 arrays of plottable data\n",
        "trn_Loss = []\n",
        "val_Loss = []\n",
        "\n",
        "trn_Accr = []\n",
        "val_Accr = []\n",
        "\n",
        "Training_Results = pd.read_csv('Training_final.csv')\n",
        "Validation_Results = pd.read_csv('Validation_final.csv')\n",
        "\n",
        "\n",
        "\n",
        "for index, item in enumerate(Training_Results[\"Training loss\"]):\n",
        "\n",
        "  trn_Loss.append(item)\n",
        "  val_Loss.append(Validation_Results[\"Validation loss epoch\"][index])\n",
        "\n",
        "  trn_Accr.append(Training_Results[\"Training accuracy\"][index])\n",
        "  val_Accr.append(Validation_Results[\"Validation accuracy epoch\"][index])\n",
        "\n",
        "\n",
        "\n",
        "# Attempt to plot data that's present\n",
        "\n",
        "epochs = range(0,10)\n",
        "# Plot and label the training and validation loss values\n",
        "plt.plot(epochs, trn_Loss, label='Training Loss')\n",
        "plt.plot(epochs, val_Loss, label='Validation Loss')\n",
        "\n",
        "# Add in a title and axes labels\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "# Set the tick locations\n",
        "# plt.xticks(range(1, 6, 1))\n",
        "\n",
        "# Display the plot\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "KamBHaf0mwNV"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot and label the training and validation loss values\n",
        "plt.plot(epochs, trn_Accr, label='Training Accuracy')\n",
        "plt.plot(epochs, val_Accr, label='Validation Accuracy')\n",
        "\n",
        "# Add in a title and axes labels\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "# Set the tick locations\n",
        "# plt.xticks(arange(1, 6, 1))\n",
        "\n",
        "# Display the plot\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Gz-wHAw3xMk"
      },
      "source": [
        "## **Inference**\n",
        "\n",
        "### try random sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "zPDla1mmZiax",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "state of florida has a city called orlando . on a monday january 2nd , the disney employees will give a speech\n",
            "['O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'B-tim', 'I-tim', 'I-tim', 'O', 'O', 'B-org', 'O', 'O', 'O', 'O', 'O']\n"
          ]
        }
      ],
      "source": [
        "sentence = \"State of Florida has a city called Orlando. On a Monday January 2nd, the Disney employees will give a speech\"\n",
        "\n",
        "inputs = tokenizer(sentence, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
        "\n",
        "# move to gpu\n",
        "ids = inputs[\"input_ids\"].to(device)\n",
        "mask = inputs[\"attention_mask\"].to(device)\n",
        "# forward pass\n",
        "outputs = model(ids, mask)\n",
        "logits = outputs[0]\n",
        "\n",
        "active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
        "token_predictions = [id2label[i] for i in flattened_predictions.cpu().numpy()]\n",
        "wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
        "\n",
        "word_level_predictions = []\n",
        "for pair in wp_preds:\n",
        "  if (pair[0].startswith(\" ##\")) or (pair[0] in ['[CLS]', '[SEP]', '[PAD]']):\n",
        "    # skip prediction\n",
        "    continue\n",
        "  else:\n",
        "    word_level_predictions.append(pair[1])\n",
        "\n",
        "# we join tokens, if they are not special ones\n",
        "str_rep = \" \".join([t[0] for t in wp_preds if t[0] not in ['[CLS]', '[SEP]', '[PAD]']]).replace(\" ##\", \"\")\n",
        "print(str_rep)\n",
        "print(word_level_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDIqKc-9cXBl"
      },
      "source": [
        "### Make a pipline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "D5KB5TKRcdRT"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'entity_group': 'org',\n",
              "  'score': 0.6873073,\n",
              "  'word': 'sahara',\n",
              "  'start': None,\n",
              "  'end': None},\n",
              " {'entity_group': 'geo',\n",
              "  'score': 0.99400985,\n",
              "  'word': 'orlando',\n",
              "  'start': None,\n",
              "  'end': None}]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(task=\"token-classification\", model=model.to(\"cpu\"), tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
        "pipe(\"My name is Sahara and Orlando is a city\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Results and Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our approach to training this model stared with performing an analysis on our dataset to identify relevant distributions and/or data features that could potentially affect our model. Our EDA revealed that the dataset did contain a number of features we were looking for and that would be useful for our chosen purpose of utilizing this model for identifying relevant information from news articles.\n",
        "The data was split up using a standard 80% Training 20% validation split. Due to the requirement for BERT needing an equal length input, we initialized a method to trim up or down based on the decided max length.\n",
        "Our initial parameters included a Max Length of 128, a Training Batch Size of 4, a Validation batch size of 2, on one Epoch and with a learning rate of 1*10^-5. While the results gathered from this initial run were sufficient to support the observation that BERT can operate relatively well with few learning epochs we deiced to tune this value as much as our hardware would allow to find the most optimal performance level.\n",
        "\n",
        "After consecutive Training, we were able reduce the training loss to 0.7% and increase the training accuracy to 98%. While this is definitely the best we’ve gotten for these values with how many epochs we ran, a graph of our Training vs Validation loss would likely suggest overfitting at anything above 4 epochs. (This accuracy was obtained at Epoch 6)\n",
        "This leads us to believe that the actual optimal method for training of this model hovers roughly around 2 Epochs as that’s the inflection point where we get the most amount of accuracy without the overfitting that can lead to further losses in training. It’s important to note that optimization of the training at around 2 epochs is much more preferred as you can obtain a high level of performance at this level with minimal resource utilization. You can technically train this model up to 4 epochs to get a large increase in accuracy with minimal hit to Validation loss and accuracy. The resources required to do this however may make it cost prohibitive.\n",
        "We were able to test our model with inferencing on identifying locations time people and organizations and it was found that it was able to perform these tasks with relative success, when trained with one epoch. Due to limitations in hardware we were unable to train past 6 epochs, however given the models performance on one, and optimization to 2, we can infer that it would only perform moderately better with 1 level of additional training, after which the model should be sufficient for our use in identifying features in news articles and large information sets.  \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "gyGRy7VrH5g3",
        "ez7qlFHl56ZW",
        "GzrPkRKlJEc1"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
